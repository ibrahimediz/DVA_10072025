"""
# GeliÅŸmiÅŸ Word Embedding YÃ¶ntemleri - Word2Vec, GloVe ve FastText

## Word Embedding YÃ¶ntemleri KarÅŸÄ±laÅŸtÄ±rmasÄ±

### 1. Word2Vec
- **GeliÅŸtirici**: Google (Mikolov et al., 2013)
- **YaklaÅŸÄ±m**: Neural network tabanlÄ±
- **Modeller**: CBOW (Continuous Bag of Words) ve Skip-gram
- **Avantajlar**: HÄ±zlÄ±, etkili, anlamsal iliÅŸkileri iyi yakalar
- **Dezavantajlar**: OOV (Out-of-Vocabulary) kelimelerle sorun

### 2. GloVe (Global Vectors)
- **GeliÅŸtirici**: Stanford (Pennington et al., 2014)
- **YaklaÅŸÄ±m**: Global istatistiksel bilgi + yerel baÄŸlam
- **Ã–zellik**: Co-occurrence matrisini kullanÄ±r
- **Avantajlar**: Global ve yerel bilgiyi birleÅŸtirir
- **Dezavantajlar**: BÃ¼yÃ¼k veri setlerinde yavaÅŸ

### 3. FastText
- **GeliÅŸtirici**: Facebook AI (Bojanowski et al., 2017)
- **YaklaÅŸÄ±m**: Subword information (alt-kelime bilgisi)
- **Ã–zellik**: Karakter n-gramlarÄ± kullanÄ±r
- **Avantajlar**: OOV kelimelerle baÅŸa Ã§Ä±kabilir, morfolojik zengin diller iÃ§in ideal
- **Dezavantajlar**: Daha fazla bellek kullanÄ±r

## TÃ¼rkÃ§e iÃ§in Ã–neriler
- **FastText**: TÃ¼rkÃ§e'nin morfolojik yapÄ±sÄ± iÃ§in en uygun
- **Word2Vec**: HÄ±z ve basitlik iÃ§in
- **GloVe**: BÃ¼yÃ¼k korpuslar iÃ§in global bilgi gerektiÄŸinde

## KullanÄ±m AlanlarÄ±
- Metin sÄ±nÄ±flandÄ±rma
- Duygu analizi
- Makine Ã§evirisi
- Bilgi Ã§Ä±karÄ±mÄ±
- Ã–neri sistemleri
"""

import numpy as np
import pandas as pd
from collections import Counter, defaultdict
import re
import warnings
warnings.filterwarnings('ignore')

# GÃ¶rselleÅŸtirme iÃ§in
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics.pairwise import cosine_similarity

print("=== GELÄ°ÅžMÄ°Åž WORD EMBEDDING YÃ–NTEMLERÄ° ===\n")

# Daha kapsamlÄ± TÃ¼rkÃ§e Ã¶rnek metin
ornek_metin = """
TÃ¼rkiye Cumhuriyeti'nin baÅŸkenti Ankara'dÄ±r. Ä°stanbul ise nÃ¼fus bakÄ±mÄ±ndan en bÃ¼yÃ¼k ÅŸehirdir.
Karadeniz, Akdeniz ve Ege denizleri TÃ¼rkiye'yi Ã§evreler. BoÄŸaziÃ§i Ä°stanbul'u ikiye bÃ¶ler.
Anadolu ve Trakya olmak Ã¼zere iki ana bÃ¶lgesi vardÄ±r. TÃ¼rk mutfaÄŸÄ± dÃ¼nyaca Ã¼nlÃ¼dÃ¼r.
Kebap, dÃ¶ner, baklava, kÃ¼nefe ve TÃ¼rk kahvesi meÅŸhur lezzetlerdir. Ä°stanbul'da Galata Kulesi,
Ayasofya ve Sultanahmet Camii Ã¶nemli tarihi yapÄ±lardÄ±r. Kapadokya'da peri bacalarÄ± ve
yeraltÄ± ÅŸehirleri bulunur. Pamukkale'nin beyaz travertenleri Ã§ok gÃ¼zeldir.
TÃ¼rkiye dÃ¶rt mevsim yaÅŸanÄ±r. Ä°lkbahar Ã§iÃ§eklerle, yaz sÄ±cakla, sonbahar yapraklarla,
kÄ±ÅŸ karla gelir. TÃ¼rk halkÄ± misafirperverdir. Ã‡ay iÃ§mek TÃ¼rk kÃ¼ltÃ¼rÃ¼nÃ¼n Ã¶nemli parÃ§asÄ±dÄ±r.
Futbol en popÃ¼ler spordur. Galatasaray, FenerbahÃ§e ve BeÅŸiktaÅŸ bÃ¼yÃ¼k takÄ±mlardÄ±r.
TÃ¼rk edebiyatÄ±nda Nazim Hikmet, Orhan Pamuk gibi Ã¶nemli yazarlar vardÄ±r.
"""

print("Ã–rnek Metin:")
print(ornek_metin[:200] + "...")
print("\n" + "="*70 + "\n")

# Metin Ã¶n iÅŸleme fonksiyonu
def preprocess_turkish_text(text):
    """
    TÃ¼rkÃ§e metin iÃ§in Ã¶n iÅŸleme
    """
    # KÃ¼Ã§Ã¼k harfe Ã§evir
    text = text.lower()
    
    # TÃ¼rkÃ§e karakterleri koru, diÄŸer noktalama iÅŸaretlerini kaldÄ±r
    text = re.sub(r'[^\w\sÃ§ÄŸÄ±Ã¶ÅŸÃ¼Ã‡ÄžIÄ°Ã–ÅžÃœ]', ' ', text)
    
    # Ã‡oklu boÅŸluklarÄ± tek boÅŸluÄŸa Ã§evir
    text = re.sub(r'\s+', ' ', text)
    
    # Kelimelere bÃ¶l ve kÄ±sa kelimeleri filtrele
    words = [word for word in text.strip().split() if len(word) > 2]
    
    return words

# Metni Ã¶n iÅŸle
words = preprocess_turkish_text(ornek_metin)
print(f"Ã–n iÅŸlenmiÅŸ kelime sayÄ±sÄ±: {len(words)}")
print(f"Benzersiz kelime sayÄ±sÄ±: {len(set(words))}")
print(f"Ä°lk 15 kelime: {words[:15]}")
print("\n" + "="*70 + "\n")

# =============================================================================
# 1. WORD2VEC Ä°MPLEMENTASYONU
# =============================================================================

print("1. WORD2VEC Ä°MPLEMENTASYONU")
print("-" * 50)

class Word2VecTurkish:
    """
    TÃ¼rkÃ§e iÃ§in basitleÅŸtirilmiÅŸ Word2Vec implementasyonu
    Skip-gram modeli kullanÄ±r
    """
    
    def __init__(self, vector_size=100, window=5, min_count=1, epochs=10):
        self.vector_size = vector_size
        self.window = window
        self.min_count = min_count
        self.epochs = epochs
        
        self.word_to_index = {}
        self.index_to_word = {}
        self.word_vectors = {}
        self.vocabulary_size = 0
        
    def build_vocabulary(self, words):
        """
        Kelime daÄŸarcÄ±ÄŸÄ±nÄ± oluÅŸturur
        """
        word_counts = Counter(words)
        
        # Min count filtresi uygula
        filtered_words = [word for word, count in word_counts.items() 
                         if count >= self.min_count]
        
        # Kelime-indeks eÅŸleÅŸtirmesi
        for i, word in enumerate(set(filtered_words)):
            self.word_to_index[word] = i
            self.index_to_word[i] = word
            
        self.vocabulary_size = len(self.word_to_index)
        print(f"Kelime daÄŸarcÄ±ÄŸÄ± boyutu: {self.vocabulary_size}")
        
    def generate_training_data(self, words):
        """
        Skip-gram iÃ§in eÄŸitim verisi oluÅŸturur
        """
        training_data = []
        
        for i, target_word in enumerate(words):
            if target_word not in self.word_to_index:
                continue
                
            # Pencere iÃ§indeki baÄŸlam kelimelerini al
            start = max(0, i - self.window)
            end = min(len(words), i + self.window + 1)
            
            for j in range(start, end):
                if i != j and words[j] in self.word_to_index:
                    training_data.append((target_word, words[j]))
        
        return training_data
    
    def train(self, words):
        """
        Word2Vec modelini eÄŸitir (basitleÅŸtirilmiÅŸ versiyon)
        """
        print("Word2Vec eÄŸitimi baÅŸlÄ±yor...")
        
        # Kelime daÄŸarcÄ±ÄŸÄ±nÄ± oluÅŸtur
        self.build_vocabulary(words)
        
        # EÄŸitim verisi oluÅŸtur
        training_data = self.generate_training_data(words)
        print(f"EÄŸitim Ã¶rneÄŸi sayÄ±sÄ±: {len(training_data)}")
        
        # Rastgele vektÃ¶rlerle baÅŸla
        for word in self.word_to_index:
            self.word_vectors[word] = np.random.uniform(-0.5, 0.5, self.vector_size)
        
        # Basit eÄŸitim dÃ¶ngÃ¼sÃ¼ (gerÃ§ek Word2Vec daha karmaÅŸÄ±k)
        for epoch in range(self.epochs):
            total_loss = 0
            
            for target_word, context_word in training_data:
                # Basit gÃ¼ncelleme (gerÃ§ekte negative sampling kullanÄ±lÄ±r)
                target_vec = self.word_vectors[target_word]
                context_vec = self.word_vectors[context_word]
                
                # Cosine similarity hesapla
                similarity = np.dot(target_vec, context_vec) / (
                    np.linalg.norm(target_vec) * np.linalg.norm(context_vec) + 1e-8
                )
                
                # Basit gradient gÃ¼ncelleme
                learning_rate = 0.01
                error = 1 - similarity
                
                # VektÃ¶rleri gÃ¼ncelle
                self.word_vectors[target_word] += learning_rate * error * context_vec
                self.word_vectors[context_word] += learning_rate * error * target_vec
                
                total_loss += error ** 2
            
            if epoch % 2 == 0:
                print(f"Epoch {epoch + 1}/{self.epochs}, Loss: {total_loss:.4f}")
        
        # VektÃ¶rleri normalize et
        for word in self.word_vectors:
            norm = np.linalg.norm(self.word_vectors[word])
            if norm > 0:
                self.word_vectors[word] = self.word_vectors[word] / norm
        
        print("Word2Vec eÄŸitimi tamamlandÄ±!")
    
    def find_similar_words(self, word, top_k=5):
        """
        Verilen kelimeye en benzer kelimeleri bulur
        """
        if word not in self.word_vectors:
            return []
        
        target_vector = self.word_vectors[word]
        similarities = []
        
        for other_word, other_vector in self.word_vectors.items():
            if other_word != word:
                similarity = np.dot(target_vector, other_vector)
                similarities.append((other_word, similarity))
        
        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities[:top_k]
    
    def word_analogy(self, word1, word2, word3):
        """
        Kelime analojisi: word1 - word2 + word3 = ?
        Ã–rnek: kral - erkek + kadÄ±n = kraliÃ§e
        """
        if not all(word in self.word_vectors for word in [word1, word2, word3]):
            return []
        
        # VektÃ¶r aritmetiÄŸi
        result_vector = (self.word_vectors[word1] - 
                        self.word_vectors[word2] + 
                        self.word_vectors[word3])
        
        # En benzer kelimeyi bul
        similarities = []
        for word, vector in self.word_vectors.items():
            if word not in [word1, word2, word3]:
                similarity = np.dot(result_vector, vector) / (
                    np.linalg.norm(result_vector) * np.linalg.norm(vector) + 1e-8
                )
                similarities.append((word, similarity))
        
        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities[:5]

# Word2Vec Ã¶rneÄŸi
print("Word2Vec modeli eÄŸitiliyor...")
word2vec_model = Word2VecTurkish(vector_size=50, window=3, epochs=5)
word2vec_model.train(words)

# Benzer kelimeleri test et
test_words = ['istanbul', 'tÃ¼rkiye', 'gÃ¼zel']
for test_word in test_words:
    if test_word in word2vec_model.word_vectors:
        similar = word2vec_model.find_similar_words(test_word)
        print(f"\n'{test_word}' kelimesine benzer kelimeler:")
        for word, similarity in similar:
            print(f"  {word}: {similarity:.3f}")

print("\n" + "="*70 + "\n")

# =============================================================================
# 2. GLOVE Ä°MPLEMENTASYONU
# =============================================================================

print("2. GLOVE (GLOBAL VECTORS) Ä°MPLEMENTASYONU")
print("-" * 50)

class GloVeTurkish:
    """
    TÃ¼rkÃ§e iÃ§in basitleÅŸtirilmiÅŸ GloVe implementasyonu
    """
    
    def __init__(self, vector_size=100, window=5, epochs=10, learning_rate=0.05):
        self.vector_size = vector_size
        self.window = window
        self.epochs = epochs
        self.learning_rate = learning_rate
        
        self.word_to_index = {}
        self.index_to_word = {}
        self.cooccurrence_matrix = {}
        self.word_vectors = {}
        self.vocabulary_size = 0
        
    def build_vocabulary(self, words):
        """
        Kelime daÄŸarcÄ±ÄŸÄ±nÄ± oluÅŸturur
        """
        word_counts = Counter(words)
        
        for i, word in enumerate(set(words)):
            self.word_to_index[word] = i
            self.index_to_word[i] = word
            
        self.vocabulary_size = len(self.word_to_index)
        print(f"GloVe kelime daÄŸarcÄ±ÄŸÄ± boyutu: {self.vocabulary_size}")
    
    def build_cooccurrence_matrix(self, words):
        """
        Co-occurrence matrisini oluÅŸturur
        """
        print("Co-occurrence matrisi oluÅŸturuluyor...")
        
        # Matrisi baÅŸlat
        for word1 in self.word_to_index:
            self.cooccurrence_matrix[word1] = defaultdict(float)
        
        # Pencere iÃ§indeki birlikte geÃ§meleri say
        for i, target_word in enumerate(words):
            if target_word not in self.word_to_index:
                continue
                
            start = max(0, i - self.window)
            end = min(len(words), i + self.window + 1)
            
            for j in range(start, end):
                if i != j and words[j] in self.word_to_index:
                    # Mesafeye gÃ¶re aÄŸÄ±rlÄ±klandÄ±r
                    distance = abs(i - j)
                    weight = 1.0 / distance
                    
                    self.cooccurrence_matrix[target_word][words[j]] += weight
        
        # Simetrik hale getir
        for word1 in self.cooccurrence_matrix:
            for word2 in self.cooccurrence_matrix[word1]:
                if word2 in self.cooccurrence_matrix:
                    avg_count = (self.cooccurrence_matrix[word1][word2] + 
                               self.cooccurrence_matrix[word2][word1]) / 2
                    self.cooccurrence_matrix[word1][word2] = avg_count
                    self.cooccurrence_matrix[word2][word1] = avg_count
    
    def weighting_function(self, x, x_max=100, alpha=0.75):
        """
        GloVe aÄŸÄ±rlÄ±klandÄ±rma fonksiyonu
        """
        if x < x_max:
            return (x / x_max) ** alpha
        else:
            return 1.0
    
    def train(self, words):
        """
        GloVe modelini eÄŸitir
        """
        print("GloVe eÄŸitimi baÅŸlÄ±yor...")
        
        # Kelime daÄŸarcÄ±ÄŸÄ±nÄ± oluÅŸtur
        self.build_vocabulary(words)
        
        # Co-occurrence matrisini oluÅŸtur
        self.build_cooccurrence_matrix(words)
        
        # Rastgele vektÃ¶rlerle baÅŸla
        W = np.random.uniform(-0.5, 0.5, (self.vocabulary_size, self.vector_size))
        W_tilde = np.random.uniform(-0.5, 0.5, (self.vocabulary_size, self.vector_size))
        b = np.random.uniform(-0.5, 0.5, self.vocabulary_size)
        b_tilde = np.random.uniform(-0.5, 0.5, self.vocabulary_size)
        
        # EÄŸitim dÃ¶ngÃ¼sÃ¼
        for epoch in range(self.epochs):
            total_loss = 0
            count = 0
            
            for word1 in self.cooccurrence_matrix:
                i = self.word_to_index[word1]
                
                for word2, x_ij in self.cooccurrence_matrix[word1].items():
                    if x_ij > 0:
                        j = self.word_to_index[word2]
                        
                        # GloVe loss hesapla
                        weight = self.weighting_function(x_ij)
                        
                        # Tahmin
                        prediction = np.dot(W[i], W_tilde[j]) + b[i] + b_tilde[j]
                        
                        # Loss
                        loss = weight * (prediction - np.log(x_ij)) ** 2
                        total_loss += loss
                        
                        # Gradient gÃ¼ncelleme (basitleÅŸtirilmiÅŸ)
                        error = prediction - np.log(x_ij)
                        grad_factor = weight * error * self.learning_rate
                        
                        # VektÃ¶rleri gÃ¼ncelle
                        W[i] -= grad_factor * W_tilde[j]
                        W_tilde[j] -= grad_factor * W[i]
                        b[i] -= grad_factor
                        b_tilde[j] -= grad_factor
                        
                        count += 1
            
            if epoch % 2 == 0:
                avg_loss = total_loss / count if count > 0 else 0
                print(f"Epoch {epoch + 1}/{self.epochs}, Avg Loss: {avg_loss:.4f}")
        
        # Final vektÃ¶rleri oluÅŸtur (W + W_tilde)
        for word in self.word_to_index:
            i = self.word_to_index[word]
            self.word_vectors[word] = (W[i] + W_tilde[i]) / 2
            
            # Normalize et
            norm = np.linalg.norm(self.word_vectors[word])
            if norm > 0:
                self.word_vectors[word] = self.word_vectors[word] / norm
        
        print("GloVe eÄŸitimi tamamlandÄ±!")
    
    def find_similar_words(self, word, top_k=5):
        """
        Benzer kelimeleri bulur
        """
        if word not in self.word_vectors:
            return []
        
        target_vector = self.word_vectors[word]
        similarities = []
        
        for other_word, other_vector in self.word_vectors.items():
            if other_word != word:
                similarity = np.dot(target_vector, other_vector)
                similarities.append((other_word, similarity))
        
        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities[:top_k]

# GloVe Ã¶rneÄŸi
print("GloVe modeli eÄŸitiliyor...")
glove_model = GloVeTurkish(vector_size=50, window=3, epochs=5)
glove_model.train(words)

# Benzer kelimeleri test et
for test_word in ['tÃ¼rkiye', 'gÃ¼zel']:
    if test_word in glove_model.word_vectors:
        similar = glove_model.find_similar_words(test_word)
        print(f"\n'{test_word}' kelimesine benzer kelimeler (GloVe):")
        for word, similarity in similar:
            print(f"  {word}: {similarity:.3f}")

print("\n" + "="*70 + "\n")

# =============================================================================
# 3. FASTTEXT Ä°MPLEMENTASYONU
# =============================================================================

print("3. FASTTEXT Ä°MPLEMENTASYONU")
print("-" * 50)

class FastTextTurkish:
    """
    TÃ¼rkÃ§e iÃ§in basitleÅŸtirilmiÅŸ FastText implementasyonu
    Subword information kullanÄ±r
    """
    
    def __init__(self, vector_size=100, window=5, min_n=3, max_n=6, epochs=5):
        self.vector_size = vector_size
        self.window = window
        self.min_n = min_n  # Minimum n-gram boyutu
        self.max_n = max_n  # Maximum n-gram boyutu
        self.epochs = epochs
        
        self.word_to_index = {}
        self.subword_to_index = {}
        self.word_vectors = {}
        self.subword_vectors = {}
        self.vocabulary_size = 0
        
    def get_subwords(self, word):
        """
        Kelimeden subword'leri (karakter n-gramlarÄ±) Ã§Ä±karÄ±r
        """
        # Kelimeyi <> ile Ã§evrele
        padded_word = f"<{word}>"
        subwords = []
        
        # N-gramlarÄ± oluÅŸtur
        for n in range(self.min_n, self.max_n + 1):
            for i in range(len(padded_word) - n + 1):
                subword = padded_word[i:i + n]
                subwords.append(subword)
        
        return subwords
    
    def build_vocabulary(self, words):
        """
        Kelime ve subword daÄŸarcÄ±ÄŸÄ±nÄ± oluÅŸturur
        """
        print("FastText kelime ve subword daÄŸarcÄ±ÄŸÄ± oluÅŸturuluyor...")
        
        # Kelime daÄŸarcÄ±ÄŸÄ±
        for i, word in enumerate(set(words)):
            self.word_to_index[word] = i
            
        self.vocabulary_size = len(self.word_to_index)
        
        # Subword daÄŸarcÄ±ÄŸÄ±
        all_subwords = set()
        for word in self.word_to_index:
            subwords = self.get_subwords(word)
            all_subwords.update(subwords)
        
        for i, subword in enumerate(all_subwords):
            self.subword_to_index[subword] = i
        
        print(f"Kelime sayÄ±sÄ±: {len(self.word_to_index)}")
        print(f"Subword sayÄ±sÄ±: {len(self.subword_to_index)}")
        
        # Ã–rnek subword'ler gÃ¶ster
        sample_word = list(self.word_to_index.keys())[0]
        sample_subwords = self.get_subwords(sample_word)
        print(f"'{sample_word}' kelimesinin subword'leri: {sample_subwords[:5]}...")
    
    def train(self, words):
        """
        FastText modelini eÄŸitir
        """
        print("FastText eÄŸitimi baÅŸlÄ±yor...")
        
        # DaÄŸarcÄ±ÄŸÄ± oluÅŸtur
        self.build_vocabulary(words)
        
        # Rastgele vektÃ¶rlerle baÅŸla
        for word in self.word_to_index:
            self.word_vectors[word] = np.random.uniform(-0.5, 0.5, self.vector_size)
        
        for subword in self.subword_to_index:
            self.subword_vectors[subword] = np.random.uniform(-0.5, 0.5, self.vector_size)
        
        # EÄŸitim dÃ¶ngÃ¼sÃ¼
        for epoch in range(self.epochs):
            total_loss = 0
            count = 0
            
            for i, target_word in enumerate(words):
                if target_word not in self.word_to_index:
                    continue
                
                # BaÄŸlam kelimelerini al
                start = max(0, i - self.window)
                end = min(len(words), i + self.window + 1)
                
                for j in range(start, end):
                    if i != j and words[j] in self.word_to_index:
                        context_word = words[j]
                        
                        # Target kelimesinin subword'lerini al
                        target_subwords = self.get_subwords(target_word)
                        
                        # Subword vektÃ¶rlerinin ortalamasÄ±nÄ± al
                        target_vector = np.zeros(self.vector_size)
                        valid_subwords = 0
                        
                        for subword in target_subwords:
                            if subword in self.subword_vectors:
                                target_vector += self.subword_vectors[subword]
                                valid_subwords += 1
                        
                        if valid_subwords > 0:
                            target_vector /= valid_subwords
                        
                        # Context kelimesinin vektÃ¶rÃ¼
                        context_vector = self.word_vectors[context_word]
                        
                        # Similarity hesapla ve gÃ¼ncelle
                        similarity = np.dot(target_vector, context_vector) / (
                            np.linalg.norm(target_vector) * np.linalg.norm(context_vector) + 1e-8
                        )
                        
                        # Basit gÃ¼ncelleme
                        learning_rate = 0.01
                        error = 1 - similarity
                        
                        # Subword vektÃ¶rlerini gÃ¼ncelle
                        for subword in target_subwords:
                            if subword in self.subword_vectors:
                                self.subword_vectors[subword] += learning_rate * error * context_vector / valid_subwords
                        
                        # Context vektÃ¶rÃ¼nÃ¼ gÃ¼ncelle
                        self.word_vectors[context_word] += learning_rate * error * target_vector
                        
                        total_loss += error ** 2
                        count += 1
            
            if epoch % 1 == 0:
                avg_loss = total_loss / count if count > 0 else 0
                print(f"Epoch {epoch + 1}/{self.epochs}, Avg Loss: {avg_loss:.4f}")
        
        # Final kelime vektÃ¶rlerini oluÅŸtur (subword vektÃ¶rlerinin ortalamasÄ±)
        for word in self.word_to_index:
            subwords = self.get_subwords(word)
            word_vector = np.zeros(self.vector_size)
            valid_subwords = 0
            
            for subword in subwords:
                if subword in self.subword_vectors:
                    word_vector += self.subword_vectors[subword]
                    valid_subwords += 1
            
            if valid_subwords > 0:
                word_vector /= valid_subwords
                # Normalize et
                norm = np.linalg.norm(word_vector)
                if norm > 0:
                    word_vector = word_vector / norm
                
                self.word_vectors[word] = word_vector
        
        print("FastText eÄŸitimi tamamlandÄ±!")
    
    def get_word_vector(self, word):
        """
        Kelime vektÃ¶rÃ¼nÃ¼ dÃ¶ndÃ¼rÃ¼r (OOV kelimeler iÃ§in subword'lerden hesaplar)
        """
        if word in self.word_vectors:
            return self.word_vectors[word]
        
        # OOV kelime iÃ§in subword'lerden vektÃ¶r oluÅŸtur
        subwords = self.get_subwords(word)
        word_vector = np.zeros(self.vector_size)
        valid_subwords = 0
        
        for subword in subwords:
            if subword in self.subword_vectors:
                word_vector += self.subword_vectors[subword]
                valid_subwords += 1
        
        if valid_subwords > 0:
            word_vector /= valid_subwords
            norm = np.linalg.norm(word_vector)
            if norm > 0:
                word_vector = word_vector / norm
        
        return word_vector
    
    def find_similar_words(self, word, top_k=5):
        """
        Benzer kelimeleri bulur (OOV kelimeler dahil)
        """
        target_vector = self.get_word_vector(word)
        
        if np.linalg.norm(target_vector) == 0:
            return []
        
        similarities = []
        
        for other_word in self.word_to_index:
            if other_word != word:
                other_vector = self.word_vectors[other_word]
                similarity = np.dot(target_vector, other_vector)
                similarities.append((other_word, similarity))
        
        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities[:top_k]

# FastText Ã¶rneÄŸi
print("FastText modeli eÄŸitiliyor...")
fasttext_model = FastTextTurkish(vector_size=50, window=3, epochs=3)
fasttext_model.train(words)

# Benzer kelimeleri test et
for test_word in ['istanbul', 'gÃ¼zel']:
    if test_word in fasttext_model.word_to_index:
        similar = fasttext_model.find_similar_words(test_word)
        print(f"\n'{test_word}' kelimesine benzer kelimeler (FastText):")
        for word, similarity in similar:
            print(f"  {word}: {similarity:.3f}")

# OOV kelime testi
oov_word = "istanbullu"  # EÄŸitim setinde olmayan kelime
print(f"\nOOV kelime testi: '{oov_word}'")
oov_vector = fasttext_model.get_word_vector(oov_word)
if np.linalg.norm(oov_vector) > 0:
    print(f"OOV kelime iÃ§in vektÃ¶r oluÅŸturuldu (norm: {np.linalg.norm(oov_vector):.3f})")
    similar_to_oov = fasttext_model.find_similar_words(oov_word)
    print(f"'{oov_word}' kelimesine benzer kelimeler:")
    for word, similarity in similar_to_oov:
        print(f"  {word}: {similarity:.3f}")

print("\n" + "="*70 + "\n")

# =============================================================================
# 4. EK KÃœTÃœPHANE: TRANSFORMERS Ä°LE WORD EMBEDDING
# =============================================================================

print("4. EK KÃœTÃœPHANE: TRANSFORMERS Ä°LE WORD EMBEDDING")
print("-" * 50)

try:
    from transformers import AutoTokenizer, AutoModel
    import torch
    
    class TransformersWordEmbedding:
        """
        Hugging Face Transformers ile Ã¶nceden eÄŸitilmiÅŸ modeller
        """
        
        def __init__(self, model_name='dbmdz/bert-base-turkish-cased'):
            try:
                print(f"Model yÃ¼kleniyor: {model_name}")
                self.tokenizer = AutoTokenizer.from_pretrained(model_name)
                self.model = AutoModel.from_pretrained(model_name)
                self.model.eval()
                self.model_loaded = True
                print("Model baÅŸarÄ±yla yÃ¼klendi!")
            except Exception as e:
                print(f"Model yÃ¼klenemedi: {e}")
                self.model_loaded = False
        
        def get_word_embeddings(self, text):
            """
            Metindeki kelimeler iÃ§in BERT embeddings Ã§Ä±karÄ±r
            """
            if not self.model_loaded:
                return {}
            
            # Tokenize et
            inputs = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True)
            
            # Model Ã§Ä±ktÄ±sÄ±nÄ± al
            with torch.no_grad():
                outputs = self.model(**inputs)
                last_hidden_states = outputs.last_hidden_state
            
            # Token'larÄ± kelimelerle eÅŸleÅŸtir
            tokens = self.tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
            embeddings = {}
            
            for i, token in enumerate(tokens):
                if token not in ['[CLS]', '[SEP]', '[PAD]'] and not token.startswith('##'):
                    # Token'Ä± temizle
                    clean_token = token.lower()
                    if len(clean_token) > 2:
                        embedding = last_hidden_states[0][i].numpy()
                        embeddings[clean_token] = embedding
            
            return embeddings
        
        def find_similar_words_bert(self, word, embeddings, top_k=5):
            """
            BERT embeddings kullanarak benzer kelimeleri bulur
            """
            if word not in embeddings:
                return []
            
            target_vector = embeddings[word]
            similarities = []
            
            for other_word, other_vector in embeddings.items():
                if other_word != word:
                    # Cosine similarity
                    similarity = np.dot(target_vector, other_vector) / (
                        np.linalg.norm(target_vector) * np.linalg.norm(other_vector)
                    )
                    similarities.append((other_word, similarity))
            
            similarities.sort(key=lambda x: x[1], reverse=True)
            return similarities[:top_k]
    
    # Transformers Ã¶rneÄŸi
    transformer_embedder = TransformersWordEmbedding()
    
    if transformer_embedder.model_loaded:
        # BERT embeddings Ã§Ä±kar
        bert_embeddings = transformer_embedder.get_word_embeddings(ornek_metin[:200])
        print(f"BERT embeddings sayÄ±sÄ±: {len(bert_embeddings)}")
        
        if bert_embeddings:
            print(f"BERT embedding boyutu: {list(bert_embeddings.values())[0].shape}")
            
            # Benzer kelimeleri bul
            sample_word = list(bert_embeddings.keys())[0]
            similar_bert = transformer_embedder.find_similar_words_bert(sample_word, bert_embeddings)
            print(f"\n'{sample_word}' kelimesine benzer kelimeler (BERT):")
            for word, similarity in similar_bert:
                print(f"  {word}: {similarity:.3f}")

except ImportError:
    print("Transformers kÃ¼tÃ¼phanesi yÃ¼klÃ¼ deÄŸil.")
    print("Kurulum iÃ§in: pip install transformers torch")
    print("\nTransformers ile yapÄ±labilecek iÅŸlemler:")
    print("- BERT, RoBERTa, GPT gibi Ã¶nceden eÄŸitilmiÅŸ modeller")
    print("- Ã‡ok dilli ve TÃ¼rkÃ§e Ã¶zel modeller")
    print("- Contextual embeddings (baÄŸlama duyarlÄ±)")
    print("- Fine-tuning imkanÄ±")

print("\n" + "="*70 + "\n")

# =============================================================================
# 5. MODEL KARÅžILAÅžTIRMASI VE VÄ°ZUALÄ°ZASYON
# =============================================================================

print("5. MODEL KARÅžILAÅžTIRMASI VE VÄ°ZUALÄ°ZASYON")
print("-" * 50)

def visualize_embeddings(embeddings_dict, title, max_words=20):
    """
    Word embeddings'leri 2D'de gÃ¶rselleÅŸtirir
    """
    if not embeddings_dict:
        print(f"{title} iÃ§in embedding bulunamadÄ±.")
        return
    
    # Sadece ilk max_words kelimeyi al
    words = list(embeddings_dict.keys())[:max_words]
    vectors = [embeddings_dict[word] for word in words]
    
    if len(vectors) < 2:
        print(f"{title} iÃ§in yeterli embedding yok.")
        return
    
    # PCA ile boyut azaltma
    pca = PCA(n_components=2)
    vectors_2d = pca.fit_transform(vectors)
    
    # GÃ¶rselleÅŸtirme
    plt.figure(figsize=(12, 8))
    plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], alpha=0.7)
    
    # Kelime etiketleri
    for i, word in enumerate(words):
        plt.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]), 
                    xytext=(5, 5), textcoords='offset points', fontsize=9)
    
    plt.title(f'{title} - Word Embeddings Visualization (PCA)')
    plt.xlabel('PC1')
    plt.ylabel('PC2')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

# Model karÅŸÄ±laÅŸtÄ±rma tablosu
comparison_data = {
    'Model': ['Word2Vec', 'GloVe', 'FastText', 'BERT/Transformers'],
    'YaklaÅŸÄ±m': ['Neural Network', 'Matrix Factorization', 'Subword + Neural', 'Transformer'],
    'OOV Handling': ['HayÄ±r', 'HayÄ±r', 'Evet', 'Evet'],
    'TÃ¼rkÃ§e Uygunluk': ['Ä°yi', 'Ä°yi', 'MÃ¼kemmel', 'MÃ¼kemmel'],
    'EÄŸitim HÄ±zÄ±': ['HÄ±zlÄ±', 'Orta', 'Orta', 'YavaÅŸ'],
    'Bellek KullanÄ±mÄ±': ['Az', 'Orta', 'Fazla', 'Ã‡ok Fazla'],
    'Contextual': ['HayÄ±r', 'HayÄ±r', 'HayÄ±r', 'Evet']
}

comparison_df = pd.DataFrame(comparison_data)
print("MODEL KARÅžILAÅžTIRMASI:")
print(comparison_df.to_string(index=False))

# Embedding kalitesi karÅŸÄ±laÅŸtÄ±rmasÄ±
print(f"\nEMBEDDÄ°NG KALÄ°TESÄ° KARÅžILAÅžTIRMASI:")
print("-" * 40)

models = {
    'Word2Vec': word2vec_model.word_vectors,
    'GloVe': glove_model.word_vectors,
    'FastText': fasttext_model.word_vectors
}

test_word = 'tÃ¼rkiye'
if test_word in word2vec_model.word_vectors:
    print(f"'{test_word}' kelimesi iÃ§in benzer kelimeler:")
    
    for model_name, embeddings in models.items():
        if test_word in embeddings:
            # Basit benzerlik hesaplama
            target_vector = embeddings[test_word]
            similarities = []
            
            for word, vector in embeddings.items():
                if word != test_word:
                    similarity = np.dot(target_vector, vector)
                    similarities.append((word, similarity))
            
            similarities.sort(key=lambda x: x[1], reverse=True)
            top_similar = similarities[:3]
            
            print(f"\n{model_name}:")
            for word, sim in top_similar:
                print(f"  {word}: {sim:.3f}")

# Ã–neriler
print(f"\nÃ–NERÄ°LER VE SONUÃ‡:")
print("-" * 40)
print("ðŸŽ¯ TÃœRKÃ‡E Ä°Ã‡Ä°N EN Ä°YÄ° SEÃ‡Ä°MLER:")
print("  â€¢ FastText: Morfolojik zenginlik ve OOV handling")
print("  â€¢ BERT: Contextual embeddings ve yÃ¼ksek kalite")
print("  â€¢ Word2Vec: HÄ±z ve basitlik gerektiÄŸinde")

print(f"\nðŸ“Š KULLANIM ALANLARI:")
print("  â€¢ Metin SÄ±nÄ±flandÄ±rma: FastText veya BERT")
print("  â€¢ Duygu Analizi: BERT")
print("  â€¢ Bilgi Ã‡Ä±karÄ±mÄ±: BERT")
print("  â€¢ HÄ±zlÄ± Prototipler: Word2Vec")

print(f"\nâš¡ PERFORMANS Ä°PUÃ‡LARI:")
print("  â€¢ BÃ¼yÃ¼k veri setleri iÃ§in: Word2Vec veya FastText")
print("  â€¢ YÃ¼ksek kalite gerektiÄŸinde: BERT")
print("  â€¢ OOV kelimeler varsa: FastText veya BERT")
print("  â€¢ Bellek kÄ±sÄ±tlÄ± ortamlar: Word2Vec")

print(f"\nðŸ”§ UYGULAMA Ã–NERÄ°LERÄ°:")
print("  â€¢ Preprocessing: TÃ¼rkÃ§e karakterleri koru")
print("  â€¢ Corpus boyutu: En az 1M kelime Ã¶neriliyor")
print("  â€¢ Hyperparameter tuning: Grid search kullan")
print("  â€¢ Evaluation: Intrinsic ve extrinsic metrikler")

print("\n" + "="*70)
print("Word Embedding eÄŸitimi tamamlandÄ±! ðŸŽ‰")
print("="*70)