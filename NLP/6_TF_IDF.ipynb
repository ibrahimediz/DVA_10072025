{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4d2d4af",
   "metadata": {},
   "source": [
    "## TF-IDF Nedir?\n",
    "\n",
    "**TF-IDF**, bir kelimenin bir dokümanda ne kadar önemli olduğunu ölçen bir vektörleştirme yöntemidir.  \n",
    "- **TF (Term Frequency):** Bir kelimenin dokümanda kaç kez geçtiği.\n",
    "- **IDF (Inverse Document Frequency):** O kelimenin tüm dokümanlar arasında ne kadar nadir olduğu.\n",
    "\n",
    "TF-IDF, sık geçen ama her dokümanda olan kelimelere düşük, nadir ve ayırt edici kelimelere yüksek değer verir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed218184",
   "metadata": {},
   "source": [
    "Örnek Metinler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4360b0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Bugün hava çok güzel ve güneşli.\",\n",
    "    \"Yarın hava yağmurlu olacak.\",\n",
    "    \"Bugün parka gittik ve çok eğlendik.\",\n",
    "    \"Yarın da parka gitmek istiyoruz.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ca68de",
   "metadata": {},
   "source": [
    "1. scikit-learn ile TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0774e971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kelimeler (Vocab): ['bugün' 'da' 'eğlendik' 'gitmek' 'gittik' 'güneşli' 'güzel' 'hava'\n",
      " 'istiyoruz' 'olacak' 'parka' 've' 'yarın' 'yağmurlu' 'çok']\n",
      "TF-IDF Vektörleri:\n",
      " [[0.37222485 0.         0.         0.         0.         0.47212003\n",
      "  0.47212003 0.37222485 0.         0.         0.         0.37222485\n",
      "  0.         0.         0.37222485]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.43779123 0.         0.55528266 0.         0.\n",
      "  0.43779123 0.55528266 0.        ]\n",
      " [0.37222485 0.         0.47212003 0.         0.47212003 0.\n",
      "  0.         0.         0.         0.         0.37222485 0.37222485\n",
      "  0.         0.         0.37222485]\n",
      " [0.         0.48546061 0.         0.48546061 0.         0.\n",
      "  0.         0.         0.48546061 0.         0.38274272 0.\n",
      "  0.38274272 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(lowercase=True)\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "print(\"Kelimeler (Vocab):\", vectorizer.get_feature_names_out())\n",
    "print(\"TF-IDF Vektörleri:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e986be3e",
   "metadata": {},
   "source": [
    "2. Stopwords ile TF-IDF\n",
    "* Türkçe stopwords’leri kaldırarak daha anlamlı TF-IDF vektörleri elde edebilirsin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f5ced13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords çıkarıldıktan sonra kelimeler: ['eğlendik' 'gitmek' 'gittik' 'güneşli' 'güzel' 'hava' 'istiyoruz'\n",
      " 'olacak' 'parka' 'yağmurlu']\n",
      "TF-IDF Vektörleri:\n",
      " [[0.         0.         0.         0.61761437 0.61761437 0.48693426\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.48693426\n",
      "  0.         0.61761437 0.         0.61761437]\n",
      " [0.61761437 0.         0.61761437 0.         0.         0.\n",
      "  0.         0.         0.48693426 0.        ]\n",
      " [0.         0.61761437 0.         0.         0.         0.\n",
      "  0.61761437 0.         0.48693426 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "turkish_stopwords = [\n",
    "    've', 'bir', 'bu', 'şu', 'o', 'ben', 'sen', 'biz', 'siz', 'onlar',\n",
    "    'için', 'ile', 'da', 'de', 'ta', 'te', 'den', 'dan', 'ten', 'tan',\n",
    "    'nin', 'nın', 'nun', 'nün', 'in', 'ın', 'un', 'ün',\n",
    "    'ya', 'ye', 'na', 'ne', 'a', 'e', 'ı', 'i', 'u', 'ü', 'o', 'ö',\n",
    "    'çok', 'daha', 'en', 'hem', 'hiç', 'her', 'hangi', 'kendi',\n",
    "    'sonra', 'önce', 'şimdi', 'bugün', 'yarın', 'ama', 'fakat', 'ancak', 'lakin', 'ki', 'eğer', 'ise',\n",
    "    'gibi', 'kadar', 'göre', 'doğru', 'karşı', 'rağmen',\n",
    "    'mi', 'mı', 'mu', 'mü', 'midir', 'mıdır', 'mudur', 'müdür'\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer(lowercase=True, stop_words=turkish_stopwords)\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "print(\"Stopwords çıkarıldıktan sonra kelimeler:\", vectorizer.get_feature_names_out())\n",
    "print(\"TF-IDF Vektörleri:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd483a7",
   "metadata": {},
   "source": [
    "3. Lemmatize Edilmiş TF-IDF (Zeyrek veya TRNLP ile)\n",
    "* Önce her dokümandaki kelimeleri lemmatize edip, sonra TF-IDF uygulayabilirsin.\n",
    "* Aşağıda Zeyrek ile örnek var (TRNLP için yukarıdaki BoW örneğindeki gibi lemmatize fonksiyonunu kullanabilirsin):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3b629c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "APPENDING RESULT: <(bugün_Adv)(-)(bugün:advRoot_ST)>\n",
      "APPENDING RESULT: <(bugün_Noun_Time)(-)(bugün:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(hava_Adj)(-)(hava:adjectiveRoot_ST)>\n",
      "APPENDING RESULT: <(hav_Noun)(-)(hav:noun_S + a3sg_S + pnon_S + a:dat_ST)>\n",
      "APPENDING RESULT: <(hava_Noun)(-)(hava:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(çok_Postp_PCAbl)(-)(çok:postpRoot_ST)>\n",
      "APPENDING RESULT: <(çok_Adj)(-)(çok:adjectiveRoot_ST)>\n",
      "APPENDING RESULT: <(çok_Det)(-)(çok:detRoot_ST)>\n",
      "APPENDING RESULT: <(çok_Adv)(-)(çok:advRoot_ST)>\n",
      "APPENDING RESULT: <(güzel_Adj)(-)(güzel:adjectiveRoot_ST)>\n",
      "APPENDING RESULT: <(güzel_Adv)(-)(güzel:advRoot_ST)>\n",
      "APPENDING RESULT: <(güzel_Noun)(-)(güzel:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(ve_Conj)(-)(ve:conjRoot_ST)>\n",
      "APPENDING RESULT: <(güneş_Noun)(-)(güneş:noun_S + a3sg_S + pnon_S + nom_ST + li:with_S + adjectiveRoot_ST)>\n",
      "APPENDING RESULT: <(Güneş_Noun_Prop)(-)(güneş:nounProper_S + a3sg_S + pnon_S + nom_ST + li:with_S + adjectiveRoot_ST)>\n",
      "APPENDING RESULT: <(yarın_Adv)(-)(yarın:advRoot_ST)>\n",
      "APPENDING RESULT: <(yarmak_Verb)(-)(yar:verbRoot_S + vImp_S + ın:vA2pl_ST)>\n",
      "APPENDING RESULT: <(yar_Noun)(-)(yar:noun_S + a3sg_S + pnon_S + ın:gen_ST)>\n",
      "APPENDING RESULT: <(yar_Noun)(-)(yar:noun_S + a3sg_S + ın:p2sg_S + nom_ST)>\n",
      "APPENDING RESULT: <(yarı_Noun)(-)(yarı:noun_S + a3sg_S + n:p2sg_S + nom_ST)>\n",
      "APPENDING RESULT: <(yarın_Noun_Time)(-)(yarın:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(yarı_Adj)(-)(yarı:adjectiveRoot_ST + adjZeroDeriv_S + noun_S + a3sg_S + n:p2sg_S + nom_ST)>\n",
      "APPENDING RESULT: <(hava_Adj)(-)(hava:adjectiveRoot_ST)>\n",
      "APPENDING RESULT: <(hav_Noun)(-)(hav:noun_S + a3sg_S + pnon_S + a:dat_ST)>\n",
      "APPENDING RESULT: <(hava_Noun)(-)(hava:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(yağmur_Noun)(-)(yağmur:noun_S + a3sg_S + pnon_S + nom_ST + lu:with_S + adjectiveRoot_ST)>\n",
      "APPENDING RESULT: <(olmak_Verb)(-)(ol:verbRoot_S + acak:vFutPart_S + adjAfterVerb_S + aPnon_ST)>\n",
      "APPENDING RESULT: <(bugün_Adv)(-)(bugün:advRoot_ST)>\n",
      "APPENDING RESULT: <(bugün_Noun_Time)(-)(bugün:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(park_Noun)(-)(park:noun_S + a3sg_S + pnon_S + a:dat_ST)>\n",
      "APPENDING RESULT: <(Park_Noun_Prop)(-)(park:nounProper_S + a3sg_S + pnon_S + a:dat_ST)>\n",
      "APPENDING RESULT: <(parka_Noun)(-)(parka:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(gitmek_Verb)(-)(git:verbRoot_S + ti:vPast_S + k:vA1pl_ST)>\n",
      "APPENDING RESULT: <(gitmek_Verb)(-)(git:verbRoot_S + tik:vPastPart_S + adjAfterVerb_S + aPnon_ST)>\n",
      "APPENDING RESULT: <(gitmek_Verb)(-)(git:verbRoot_S + tik:vPastPart_S + noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(ve_Conj)(-)(ve:conjRoot_ST)>\n",
      "APPENDING RESULT: <(çok_Postp_PCAbl)(-)(çok:postpRoot_ST)>\n",
      "APPENDING RESULT: <(çok_Adj)(-)(çok:adjectiveRoot_ST)>\n",
      "APPENDING RESULT: <(çok_Det)(-)(çok:detRoot_ST)>\n",
      "APPENDING RESULT: <(çok_Adv)(-)(çok:advRoot_ST)>\n",
      "APPENDING RESULT: <(eğlenmek_Verb)(-)(eğlen:verbRoot_S + di:vPast_S + k:vA1pl_ST)>\n",
      "APPENDING RESULT: <(eğlenmek_Verb)(-)(eğlen:verbRoot_S + dik:vPastPart_S + adjAfterVerb_S + aPnon_ST)>\n",
      "APPENDING RESULT: <(eğlemek_Verb)(-)(eğle:verbRoot_S + n:vPass_S + verbRoot_S + di:vPast_S + k:vA1pl_ST)>\n",
      "APPENDING RESULT: <(eğlemek_Verb)(-)(eğle:verbRoot_S + n:vPass_S + verbRoot_S + dik:vPastPart_S + adjAfterVerb_S + aPnon_ST)>\n",
      "APPENDING RESULT: <(eğlenmek_Verb)(-)(eğlen:verbRoot_S + dik:vPastPart_S + noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(eğlemek_Verb)(-)(eğle:verbRoot_S + n:vPass_S + verbRoot_S + dik:vPastPart_S + noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(yarın_Adv)(-)(yarın:advRoot_ST)>\n",
      "APPENDING RESULT: <(yarmak_Verb)(-)(yar:verbRoot_S + vImp_S + ın:vA2pl_ST)>\n",
      "APPENDING RESULT: <(yar_Noun)(-)(yar:noun_S + a3sg_S + pnon_S + ın:gen_ST)>\n",
      "APPENDING RESULT: <(yar_Noun)(-)(yar:noun_S + a3sg_S + ın:p2sg_S + nom_ST)>\n",
      "APPENDING RESULT: <(yarı_Noun)(-)(yarı:noun_S + a3sg_S + n:p2sg_S + nom_ST)>\n",
      "APPENDING RESULT: <(yarın_Noun_Time)(-)(yarın:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(yarı_Adj)(-)(yarı:adjectiveRoot_ST + adjZeroDeriv_S + noun_S + a3sg_S + n:p2sg_S + nom_ST)>\n",
      "APPENDING RESULT: <(da_Conj)(-)(da:conjRoot_ST)>\n",
      "APPENDING RESULT: <(park_Noun)(-)(park:noun_S + a3sg_S + pnon_S + a:dat_ST)>\n",
      "APPENDING RESULT: <(Park_Noun_Prop)(-)(park:nounProper_S + a3sg_S + pnon_S + a:dat_ST)>\n",
      "APPENDING RESULT: <(parka_Noun)(-)(parka:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(gitmek_Verb)(-)(git:verbRoot_S + mek:vInf1_S + nounInf1Root_S + a3sgInf1_S + pnonInf1_S + nom_ST)>\n",
      "APPENDING RESULT: <(istemek_Verb)(-)(ist:verbRoot_VowelDrop_S + iyor:vProgYor_S + uz:vA1pl_ST)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeyrek ile lemmatize edilmiş kelimeler (her doküman):\n",
      "Doc 1: bugün hava çok güzel ve güneşli\n",
      "Doc 2: yarın hava yağmurlu olacak\n",
      "Doc 3: bugün parka gittik ve çok eğlendik\n",
      "Doc 4: yarın da parka gitmek istiyoruz\n",
      "TF-IDF Vektörleri (Zeyrek lemmatize):\n",
      " [[0.37222485 0.         0.         0.         0.         0.47212003\n",
      "  0.47212003 0.37222485 0.         0.         0.         0.37222485\n",
      "  0.         0.         0.37222485]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.43779123 0.         0.55528266 0.         0.\n",
      "  0.43779123 0.55528266 0.        ]\n",
      " [0.37222485 0.         0.47212003 0.         0.47212003 0.\n",
      "  0.         0.         0.         0.         0.37222485 0.37222485\n",
      "  0.         0.         0.37222485]\n",
      " [0.         0.48546061 0.         0.48546061 0.         0.\n",
      "  0.         0.         0.48546061 0.         0.38274272 0.\n",
      "  0.38274272 0.         0.        ]]\n",
      "Kelimeler (Vocab): ['bugün' 'da' 'eğlendik' 'gitmek' 'gittik' 'güneşli' 'güzel' 'hava'\n",
      " 'istiyoruz' 'olacak' 'parka' 've' 'yarın' 'yağmurlu' 'çok']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "try:\n",
    "    import zeyrek\n",
    "    analyzer = zeyrek.MorphAnalyzer()\n",
    "    zeyrek_available = True\n",
    "except ImportError:\n",
    "    print(\"Zeyrek kurulu değil.\")\n",
    "    zeyrek_available = False\n",
    "\n",
    "def lemmatize_with_zeyrek(text):\n",
    "    words = re.findall(r'\\w+', text.lower(), re.UNICODE)\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            analyses = analyzer.analyze(word)\n",
    "            if analyses and len(analyses) > 0 and len(analyses[0]) > 1 and analyses[0][1]:\n",
    "                lemma = analyses[0][1][0]\n",
    "            else:\n",
    "                lemma = word\n",
    "            # Boş string veya None ise ekleme\n",
    "            if lemma and lemma.strip():\n",
    "                lemmas.append(lemma)\n",
    "        except Exception:\n",
    "            lemmas.append(word)\n",
    "    return lemmas\n",
    "\n",
    "if zeyrek_available:\n",
    "    lemmatized_docs = [' '.join(lemmatize_with_zeyrek(doc)) for doc in documents]\n",
    "    # Boş dokümanları at\n",
    "    lemmatized_docs = [doc for doc in lemmatized_docs if doc.strip()]\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(lemmatized_docs)\n",
    "    print(\"Zeyrek ile lemmatize edilmiş kelimeler (her doküman):\")\n",
    "    for i, doc in enumerate(lemmatized_docs):\n",
    "        print(f\"Doc {i+1}: {doc}\")\n",
    "    print(\"TF-IDF Vektörleri (Zeyrek lemmatize):\\n\", X.toarray())\n",
    "    print(\"Kelimeler (Vocab):\", vectorizer.get_feature_names_out())\n",
    "else:\n",
    "    print(\"Zeyrek kurulu değil veya import edilemedi.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7313b0e",
   "metadata": {},
   "source": [
    "4. NLTK ile TF-IDF\n",
    "* NLTK’da doğrudan TF-IDF yoktur, ama kelimeleri tokenize edip scikit-learn ile kullanabilirsin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5929bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK ile tokenize edilmiş kelimeler: ['bugün' 'da' 'eğlendik' 'gitmek' 'gittik' 'güneşli' 'güzel' 'hava'\n",
      " 'istiyoruz' 'olacak' 'parka' 've' 'yarın' 'yağmurlu' 'çok']\n",
      "TF-IDF Vektörleri:\n",
      " [[0.37222485 0.         0.         0.         0.         0.47212003\n",
      "  0.47212003 0.37222485 0.         0.         0.         0.37222485\n",
      "  0.         0.         0.37222485]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.43779123 0.         0.55528266 0.         0.\n",
      "  0.43779123 0.55528266 0.        ]\n",
      " [0.37222485 0.         0.47212003 0.         0.47212003 0.\n",
      "  0.         0.         0.         0.         0.37222485 0.37222485\n",
      "  0.         0.         0.37222485]\n",
      " [0.         0.48546061 0.         0.48546061 0.         0.\n",
      "  0.         0.         0.48546061 0.         0.38274272 0.\n",
      "  0.38274272 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "tokenized_docs = [' '.join(nltk.word_tokenize(doc.lower())) for doc in documents]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(tokenized_docs)\n",
    "print(\"NLTK ile tokenize edilmiş kelimeler:\", vectorizer.get_feature_names_out())\n",
    "print(\"TF-IDF Vektörleri:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8957032c",
   "metadata": {},
   "source": [
    "5. spaCy ile TF-IDF\n",
    "* spaCy ile kelimeleri tokenize edip, scikit-learn ile TF-IDF hesaplayabilirsin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4db18c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy ile tokenize edilmiş kelimeler: ['bugün' 'da' 'eğlendik' 'gitmek' 'gittik' 'güneşli' 'güzel' 'hava'\n",
      " 'istiyoruz' 'olacak' 'parka' 've' 'yarın' 'yağmurlu' 'çok']\n",
      "TF-IDF Vektörleri:\n",
      " [[0.37222485 0.         0.         0.         0.         0.47212003\n",
      "  0.47212003 0.37222485 0.         0.         0.         0.37222485\n",
      "  0.         0.         0.37222485]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.43779123 0.         0.55528266 0.         0.\n",
      "  0.43779123 0.55528266 0.        ]\n",
      " [0.37222485 0.         0.47212003 0.         0.47212003 0.\n",
      "  0.         0.         0.         0.         0.37222485 0.37222485\n",
      "  0.         0.         0.37222485]\n",
      " [0.         0.48546061 0.         0.48546061 0.         0.\n",
      "  0.         0.         0.48546061 0.         0.38274272 0.\n",
      "  0.38274272 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"tr\")\n",
    "tokenized_docs = []\n",
    "for doc in documents:\n",
    "    tokens = [token.text.lower() for token in nlp(doc) if token.is_alpha]\n",
    "    tokenized_docs.append(' '.join(tokens))\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(tokenized_docs)\n",
    "print(\"spaCy ile tokenize edilmiş kelimeler:\", vectorizer.get_feature_names_out())\n",
    "print(\"TF-IDF Vektörleri:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9155468",
   "metadata": {},
   "source": [
    "## Pratik Notlar\n",
    "\n",
    "- **TF-IDF**, metin sınıflandırma, bilgi çıkarımı ve arama motorlarında çok kullanılır.\n",
    "- Türkçe’de lemmatizasyon ve stopwords temizliği, TF-IDF’in kalitesini artırır.\n",
    "- scikit-learn, hızlı ve pratik bir çözümdür."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
