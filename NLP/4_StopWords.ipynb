{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b1d67bd",
   "metadata": {},
   "source": [
    "# Türkçe NLP: StopWords (Durak Kelimeleri)\n",
    "\n",
    "## StopWords Nedir?\n",
    "\n",
    "**StopWords (Durak Kelimeleri)**, metinlerde çok sık geçen ancak genellikle anlamsal değeri düşük olan kelimelerdir. Örneğin: \"ve\", \"bir\", \"bu\", \"şu\", \"için\", \"ile\" gibi.\n",
    "\n",
    "### Neden Kaldırılır?\n",
    "- **Boyut azaltma**: Veri setinin boyutunu küçültür\n",
    "- **Gürültü azaltma**: Anlamsal analizi iyileştirir  \n",
    "- **Performans artışı**: İşlem hızını artırır\n",
    "- **Odaklanma**: Önemli kelimelere odaklanmayı sağlar\n",
    "\n",
    "### Ne Zaman Kaldırılmaz?\n",
    "- Duygu analizi (olumsuzluk ifadeleri önemli)\n",
    "- Makine çevirisi\n",
    "- Soru-cevap sistemleri\n",
    "- Metin özetleme\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f9ef7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orijinal metin:\n",
      "\n",
      "Bugün hava çok güzel. Ben ve arkadaşlarım parka gittik.\n",
      "Orada çok eğlendik. Futbol oynadık, koştuk ve çok güldük.\n",
      "Bu gerçekten harika bir gündü. Yarın da tekrar gideceğiz.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"\"\"\n",
    "Bugün hava çok güzel. Ben ve arkadaşlarım parka gittik.\n",
    "Orada çok eğlendik. Futbol oynadık, koştuk ve çok güldük.\n",
    "Bu gerçekten harika bir gündü. Yarın da tekrar gideceğiz.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Orijinal metin:\")\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f012d52",
   "metadata": {},
   "source": [
    "## python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99d46959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYTHON STOPWORDS KALDIRMA:\n",
      "==================================================\n",
      "Orijinal kelime sayısı: 27\n",
      "Filtrelenmiş kelime sayısı: 16\n",
      "Kaldırılan kelime sayısı: 11\n",
      "\n",
      "Orijinal kelimeler: ['bugün', 'hava', 'çok', 'güzel', 'ben', 've', 'arkadaşlarım', 'parka', 'gittik', 'orada', 'çok', 'eğlendik', 'futbol', 'oynadık', 'koştuk', 've', 'çok', 'güldük', 'bu', 'gerçekten', 'harika', 'bir', 'gündü', 'yarın', 'da', 'tekrar', 'gideceğiz']\n",
      "\n",
      "Filtrelenmiş kelimeler: ['hava', 'güzel', 'arkadaşlarım', 'parka', 'gittik', 'orada', 'eğlendik', 'futbol', 'oynadık', 'koştuk', 'güldük', 'gerçekten', 'harika', 'gündü', 'tekrar', 'gideceğiz']\n",
      "\n",
      "Kaldırılan stopwords: ['bugün', 'bir', 'bu', 'yarın', 'çok', 've', 'da', 'ben']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Türkçe stopwords listesi (manuel)\n",
    "turkish_stopwords = {\n",
    "    've', 'bir', 'bu', 'şu', 'o', 'ben', 'sen', 'biz', 'siz', 'onlar',\n",
    "    'için', 'ile', 'da', 'de', 'ta', 'te', 'den', 'dan', 'ten', 'tan',\n",
    "    'nin', 'nın', 'nun', 'nün', 'in', 'ın', 'un', 'ün',\n",
    "    'ya', 'ye', 'na', 'ne', 'a', 'e', 'ı', 'i', 'u', 'ü', 'o', 'ö',\n",
    "    'çok', 'daha', 'en', 'hem', 'hiç', 'her', 'hangi', 'kendi',\n",
    "    'sonra', 'önce', 'şimdi', 'bugün', 'yarın', 'dün',\n",
    "    'ama', 'fakat', 'ancak', 'lakin', 'ki', 'eğer', 'ise',\n",
    "    'gibi', 'kadar', 'göre', 'doğru', 'karşı', 'rağmen',\n",
    "    'mi', 'mı', 'mu', 'mü', 'midir', 'mıdır', 'mudur', 'müdür'\n",
    "}\n",
    "\n",
    "def remove_stopwords_python(text, stopwords):\n",
    "    \"\"\"Python ile stopwords kaldırma\"\"\"\n",
    "    # Metni kelimelere ayır\n",
    "    words = re.findall(r'\\w+', text.lower(), re.UNICODE)\n",
    "    \n",
    "    # Stopwords'leri kaldır\n",
    "    filtered_words = [word for word in words if word not in stopwords]\n",
    "    \n",
    "    return filtered_words, words\n",
    "\n",
    "# Test et\n",
    "filtered_python, original_words = remove_stopwords_python(sample_text, turkish_stopwords)\n",
    "\n",
    "print(\"PYTHON STOPWORDS KALDIRMA:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Orijinal kelime sayısı: {len(original_words)}\")\n",
    "print(f\"Filtrelenmiş kelime sayısı: {len(filtered_python)}\")\n",
    "print(f\"Kaldırılan kelime sayısı: {len(original_words) - len(filtered_python)}\")\n",
    "\n",
    "print(f\"\\nOrijinal kelimeler: {original_words}\")\n",
    "print(f\"\\nFiltrelenmiş kelimeler: {filtered_python}\")\n",
    "\n",
    "# Kaldırılan kelimeleri göster\n",
    "removed_words = [word for word in original_words if word in turkish_stopwords]\n",
    "print(f\"\\nKaldırılan stopwords: {list(set(removed_words))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c696b54",
   "metadata": {},
   "source": [
    "2. Zeyrek ile StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d48c89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "APPENDING RESULT: <(bugün_Adv)(-)(bugün:advRoot_ST)>\n",
      "APPENDING RESULT: <(bugün_Noun_Time)(-)(bugün:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(hava_Adj)(-)(hava:adjectiveRoot_ST)>\n",
      "APPENDING RESULT: <(hav_Noun)(-)(hav:noun_S + a3sg_S + pnon_S + a:dat_ST)>\n",
      "APPENDING RESULT: <(hava_Noun)(-)(hava:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(çok_Adj)(-)(çok:adjectiveRoot_ST)>\n",
      "APPENDING RESULT: <(çok_Det)(-)(çok:detRoot_ST)>\n",
      "APPENDING RESULT: <(çok_Adv)(-)(çok:advRoot_ST)>\n",
      "APPENDING RESULT: <(çok_Postp_PCAbl)(-)(çok:postpRoot_ST)>\n",
      "APPENDING RESULT: <(güzel_Adv)(-)(güzel:advRoot_ST)>\n",
      "APPENDING RESULT: <(güzel_Adj)(-)(güzel:adjectiveRoot_ST)>\n",
      "APPENDING RESULT: <(güzel_Noun)(-)(güzel:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(ben_Noun)(-)(ben:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(ben_Pron_Pers)(-)(ben:pronPers_S + pA1sg_S + pPnon_S + pNom_ST)>\n",
      "APPENDING RESULT: <(ve_Conj)(-)(ve:conjRoot_ST)>\n",
      "APPENDING RESULT: <(arkadaş_Noun)(-)(arkadaş:noun_S + lar:a3pl_S + ım:p1sg_S + nom_ST)>\n",
      "APPENDING RESULT: <(arkadaş_Noun)(-)(arkadaş:noun_S + lar:a3pl_S + pnon_S + nom_ST + nounZeroDeriv_S + nVerb_S + nPresent_S + ım:nA1sg_ST)>\n",
      "APPENDING RESULT: <(park_Noun)(-)(park:noun_S + a3sg_S + pnon_S + a:dat_ST)>\n",
      "APPENDING RESULT: <(Park_Noun_Prop)(-)(park:nounProper_S + a3sg_S + pnon_S + a:dat_ST)>\n",
      "APPENDING RESULT: <(parka_Noun)(-)(parka:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(gitmek_Verb)(-)(git:verbRoot_S + ti:vPast_S + k:vA1pl_ST)>\n",
      "APPENDING RESULT: <(gitmek_Verb)(-)(git:verbRoot_S + tik:vPastPart_S + adjAfterVerb_S + aPnon_ST)>\n",
      "APPENDING RESULT: <(gitmek_Verb)(-)(git:verbRoot_S + tik:vPastPart_S + noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(ora_Noun)(-)(ora:noun_S + a3sg_S + pnon_S + da:loc_ST)>\n",
      "APPENDING RESULT: <(çok_Adj)(-)(çok:adjectiveRoot_ST)>\n",
      "APPENDING RESULT: <(çok_Det)(-)(çok:detRoot_ST)>\n",
      "APPENDING RESULT: <(çok_Adv)(-)(çok:advRoot_ST)>\n",
      "APPENDING RESULT: <(çok_Postp_PCAbl)(-)(çok:postpRoot_ST)>\n",
      "APPENDING RESULT: <(eğlenmek_Verb)(-)(eğlen:verbRoot_S + di:vPast_S + k:vA1pl_ST)>\n",
      "APPENDING RESULT: <(eğlenmek_Verb)(-)(eğlen:verbRoot_S + dik:vPastPart_S + adjAfterVerb_S + aPnon_ST)>\n",
      "APPENDING RESULT: <(eğlemek_Verb)(-)(eğle:verbRoot_S + n:vPass_S + verbRoot_S + di:vPast_S + k:vA1pl_ST)>\n",
      "APPENDING RESULT: <(eğlemek_Verb)(-)(eğle:verbRoot_S + n:vPass_S + verbRoot_S + dik:vPastPart_S + adjAfterVerb_S + aPnon_ST)>\n",
      "APPENDING RESULT: <(eğlenmek_Verb)(-)(eğlen:verbRoot_S + dik:vPastPart_S + noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(eğlemek_Verb)(-)(eğle:verbRoot_S + n:vPass_S + verbRoot_S + dik:vPastPart_S + noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(futbol_Noun)(-)(futbol:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(oynamak_Verb)(-)(oyna:verbRoot_S + dı:vPast_S + k:vA1pl_ST)>\n",
      "APPENDING RESULT: <(oynamak_Verb)(-)(oyna:verbRoot_S + dık:vPastPart_S + adjAfterVerb_S + aPnon_ST)>\n",
      "APPENDING RESULT: <(oynamak_Verb)(-)(oyna:verbRoot_S + dık:vPastPart_S + noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(koşmak_Verb)(-)(koş:verbRoot_S + tu:vPast_S + k:vA1pl_ST)>\n",
      "APPENDING RESULT: <(koşmak_Verb)(-)(koş:verbRoot_S + tuk:vPastPart_S + adjAfterVerb_S + aPnon_ST)>\n",
      "APPENDING RESULT: <(koşmak_Verb)(-)(koş:verbRoot_S + tuk:vPastPart_S + noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(ve_Conj)(-)(ve:conjRoot_ST)>\n",
      "APPENDING RESULT: <(çok_Adj)(-)(çok:adjectiveRoot_ST)>\n",
      "APPENDING RESULT: <(çok_Det)(-)(çok:detRoot_ST)>\n",
      "APPENDING RESULT: <(çok_Adv)(-)(çok:advRoot_ST)>\n",
      "APPENDING RESULT: <(çok_Postp_PCAbl)(-)(çok:postpRoot_ST)>\n",
      "APPENDING RESULT: <(gülmek_Verb)(-)(gül:verbRoot_S + dü:vPast_S + k:vA1pl_ST)>\n",
      "APPENDING RESULT: <(gülmek_Verb)(-)(gül:verbRoot_S + dük:vPastPart_S + adjAfterVerb_S + aPnon_ST)>\n",
      "APPENDING RESULT: <(gülmek_Verb)(-)(gül:verbRoot_S + dük:vPastPart_S + noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(gül_Noun)(-)(gül:noun_S + a3sg_S + pnon_S + nom_ST + nounZeroDeriv_S + nVerb_S + dü:nPast_S + k:nA1pl_ST)>\n",
      "APPENDING RESULT: <(bu_Det)(-)(bu:detRoot_ST)>\n",
      "APPENDING RESULT: <(bu_Pron_Demons)(-)(bu:pronDemons_S + pA3sg_S + pPnon_S + pNom_ST)>\n",
      "APPENDING RESULT: <(gerçekten_Adv)(-)(gerçekten:advRoot_ST)>\n",
      "APPENDING RESULT: <(Gerçek_Noun_Prop)(-)(gerçek:nounProper_S + a3sg_S + pnon_S + ten:abl_ST)>\n",
      "APPENDING RESULT: <(gerçek_Noun)(-)(gerçek:noun_S + a3sg_S + pnon_S + ten:abl_ST)>\n",
      "APPENDING RESULT: <(gerçek_Adj)(-)(gerçek:adjectiveRoot_ST + adjZeroDeriv_S + noun_S + a3sg_S + pnon_S + ten:abl_ST)>\n",
      "APPENDING RESULT: <(harika_Interj)(-)(harika:interjRoot_ST)>\n",
      "APPENDING RESULT: <(harika_Adj)(-)(harika:adjectiveRoot_ST)>\n",
      "APPENDING RESULT: <(Harika_Noun_Prop)(-)(harika:nounProper_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(bir_Num_Card)(-)(bir:numeralRoot_ST)>\n",
      "APPENDING RESULT: <(bir_Det)(-)(bir:detRoot_ST)>\n",
      "APPENDING RESULT: <(bir_Adj)(-)(bir:adjectiveRoot_ST)>\n",
      "APPENDING RESULT: <(bir_Adv)(-)(bir:advRoot_ST)>\n",
      "APPENDING RESULT: <(gün_Noun_Time)(-)(gün:noun_S + a3sg_S + pnon_S + nom_ST + nounZeroDeriv_S + nVerb_S + dü:nPast_S + nA3sg_ST)>\n",
      "APPENDING RESULT: <(yarın_Adv)(-)(yarın:advRoot_ST)>\n",
      "APPENDING RESULT: <(yarmak_Verb)(-)(yar:verbRoot_S + vImp_S + ın:vA2pl_ST)>\n",
      "APPENDING RESULT: <(yar_Noun)(-)(yar:noun_S + a3sg_S + pnon_S + ın:gen_ST)>\n",
      "APPENDING RESULT: <(yar_Noun)(-)(yar:noun_S + a3sg_S + ın:p2sg_S + nom_ST)>\n",
      "APPENDING RESULT: <(yarı_Noun)(-)(yarı:noun_S + a3sg_S + n:p2sg_S + nom_ST)>\n",
      "APPENDING RESULT: <(yarın_Noun_Time)(-)(yarın:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(yarı_Adj)(-)(yarı:adjectiveRoot_ST + adjZeroDeriv_S + noun_S + a3sg_S + n:p2sg_S + nom_ST)>\n",
      "APPENDING RESULT: <(da_Conj)(-)(da:conjRoot_ST)>\n",
      "APPENDING RESULT: <(tekrar_Adv)(-)(tekrar:advRoot_ST)>\n",
      "APPENDING RESULT: <(tekrar_Noun)(-)(tekrar:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(gitmek_Verb)(-)(gid:verbRoot_S + eceğ:vFut_S + iz:vA1pl_ST)>\n",
      "APPENDING RESULT: <(Gide_Noun_Prop)(-)(gide:nounProper_S + a3sg_S + pnon_S + nom_ST + ceğiz:dim_S + noun_S + a3sg_S + pnon_S + nom_ST)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeyrek ile stopwords kaldırılmış kelimeler:\n",
      "['hava', 'güzel', 'arkadaşlarım', 'parka', 'gittik', 'orada', 'eğlendik', 'futbol', 'oynadık', 'koştuk', 'güldük', 'gerçekten', 'harika', 'gündü', 'tekrar', 'gideceğiz']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "try:\n",
    "    import zeyrek\n",
    "    analyzer = zeyrek.MorphAnalyzer()\n",
    "    zeyrek_available = True\n",
    "except ImportError:\n",
    "    print(\"Zeyrek kurulu değil.\")\n",
    "    zeyrek_available = False\n",
    "\n",
    "def remove_stopwords_zeyrek(text, stopwords):\n",
    "    if not zeyrek_available:\n",
    "        return []\n",
    "    words = re.findall(r'\\w+', text.lower(), re.UNICODE)\n",
    "    filtered_words = []\n",
    "    for word in words:\n",
    "        analyses = analyzer.analyze(word)\n",
    "        # Analiz sonucu ve lemma listesi var mı kontrolü\n",
    "        if analyses and len(analyses) > 0 and len(analyses[0]) > 1 and analyses[0][1]:\n",
    "            lemma = analyses[0][1][0]\n",
    "        else:\n",
    "            lemma = word\n",
    "        if lemma not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "    return filtered_words\n",
    "\n",
    "# Örnek metin ve stopwords listesi yukarıda tanımlı olmalı\n",
    "if zeyrek_available:\n",
    "    filtered_zeyrek = remove_stopwords_zeyrek(sample_text, turkish_stopwords)\n",
    "    print(\"Zeyrek ile stopwords kaldırılmış kelimeler:\")\n",
    "    print(filtered_zeyrek)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89515e4",
   "metadata": {},
   "source": [
    "3. TRNLP ile StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce2640b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRNLP ile stopwords kaldırılmış kelimeler:\n",
      "['hava', 'güzel', 'arkadaşlarım', 'parka', 'gittik', 'orada', 'eğlendik', 'futbol', 'oynadık', 'koştuk', 'güldük', 'gerçekten', 'harika', 'gündü', 'tekrar', 'gideceğiz']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "try:\n",
    "    from trnlp import TrnlpWord\n",
    "    trnlp_available = True\n",
    "except ImportError:\n",
    "    print(\"TRNLP kurulu değil.\")\n",
    "    trnlp_available = False\n",
    "\n",
    "def remove_stopwords_trnlp(text, stopwords):\n",
    "    if not trnlp_available:\n",
    "        return []\n",
    "    words = re.findall(r'\\w+', text.lower(), re.UNICODE)\n",
    "    filtered_words = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            tr_word = TrnlpWord()\n",
    "            tr_word.setword(word)\n",
    "            # Bazı sürümlerde lemma, bazı sürümlerde lemmas olabilir\n",
    "            lemma = getattr(tr_word, \"lemma\", None) or getattr(tr_word, \"lemmas\", None)\n",
    "            # lemma bazen None, bazen boş string, bazen liste olabilir\n",
    "            if isinstance(lemma, list):\n",
    "                lemma = lemma[0] if lemma else word\n",
    "            elif not lemma:\n",
    "                lemma = word\n",
    "        except Exception as e:\n",
    "            lemma = word\n",
    "        if lemma not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "    return filtered_words\n",
    "\n",
    "# Örnek metin ve stopwords listesi yukarıda tanımlı olmalı\n",
    "if trnlp_available:\n",
    "    filtered_trnlp = remove_stopwords_trnlp(sample_text, turkish_stopwords)\n",
    "    print(\"TRNLP ile stopwords kaldırılmış kelimeler:\")\n",
    "    print(filtered_trnlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b2b9f6",
   "metadata": {},
   "source": [
    "4. spaCy ile StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8414821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ spaCy ve Türkçe modeli kurulu\n",
      "\n",
      "\n",
      "SPACY STOPWORDS KALDIRMA:\n",
      "==================================================\n",
      "Filtrelenmiş kelime sayısı: 18\n",
      "Filtrelenmiş kelimeler: ['bugün', 'hava', 'güzel', 'arkadaşlarım', 'parka', 'gittik', 'eğlendik', 'futbol', 'oynadık', 'koştuk', 'güldük', 'gerçekten', 'harika', 'bir', 'gündü', 'yarın', 'tekrar', 'gideceğiz']\n",
      "\n",
      "Analiz detayları:\n",
      "  bugün → bugün (POS: NOUN)\n",
      "  hava → hava (POS: NOUN)\n",
      "  çok → çok (STOPWORD)\n",
      "  güzel → güzel (POS: ADJ)\n",
      "  ben → ben (STOPWORD)\n",
      "  ve → ve (STOPWORD)\n",
      "  arkadaşlarım → arkadaş (POS: NOUN)\n",
      "  parka → park (POS: NOUN)\n",
      "  gittik → git (POS: VERB)\n",
      "  orada → ora (STOPWORD)\n",
      "  çok → çok (STOPWORD)\n",
      "  eğlendik → eğlen (POS: VERB)\n",
      "  futbol → futbol (POS: NOUN)\n",
      "  oynadık → oyna (POS: VERB)\n",
      "  koştuk → koştuk (POS: VERB)\n",
      "  ve → ve (STOPWORD)\n",
      "  çok → çok (STOPWORD)\n",
      "  güldük → güldük (POS: VERB)\n",
      "  bu → bu (STOPWORD)\n",
      "  gerçekten → gerçekten (POS: ADV)\n",
      "  harika → harika (POS: ADJ)\n",
      "  bir → bir (POS: DET)\n",
      "  gündü → gündü (POS: NOUN)\n",
      "  yarın → yarın (POS: NOUN)\n",
      "  da → da (STOPWORD)\n",
      "  tekrar → tekrar (POS: ADV)\n",
      "  gideceğiz → git (POS: VERB)\n"
     ]
    }
   ],
   "source": [
    "# spaCy ile stopwords\n",
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"tr_core_news_md\")\n",
    "    spacy_available = True\n",
    "    print(\"✅ spaCy ve Türkçe modeli kurulu\")\n",
    "except:\n",
    "    print(\"❌ spaCy veya Türkçe modeli kurulu değil\")\n",
    "    spacy_available = False\n",
    "\n",
    "def remove_stopwords_spacy(text):\n",
    "    \"\"\"spaCy ile stopwords kaldırma (kendi stopwords listesi)\"\"\"\n",
    "    if not spacy_available:\n",
    "        return [], []\n",
    "    \n",
    "    doc = nlp(text.lower())\n",
    "    \n",
    "    filtered_words = []\n",
    "    analysis_info = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.is_alpha:  # Sadece alfabetik karakterler\n",
    "            if not token.is_stop:  # spaCy'nin stopword listesi\n",
    "                filtered_words.append(token.text)\n",
    "                analysis_info.append(f\"{token.text} → {token.lemma_} (POS: {token.pos_})\")\n",
    "            else:\n",
    "                analysis_info.append(f\"{token.text} → {token.lemma_} (STOPWORD)\")\n",
    "    \n",
    "    return filtered_words, analysis_info\n",
    "\n",
    "if spacy_available:\n",
    "    filtered_spacy, spacy_analysis = remove_stopwords_spacy(sample_text)\n",
    "    \n",
    "    print(\"\\n\\nSPACY STOPWORDS KALDIRMA:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Filtrelenmiş kelime sayısı: {len(filtered_spacy)}\")\n",
    "    print(f\"Filtrelenmiş kelimeler: {filtered_spacy}\")\n",
    "    \n",
    "    print(f\"\\nAnaliz detayları:\")\n",
    "    for analysis in spacy_analysis:\n",
    "        print(f\"  {analysis}\")\n",
    "else:\n",
    "    print(\"\\nspaCy kurulu olmadığı için test edilemiyor.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e521d4bd",
   "metadata": {},
   "source": [
    "5. NLTK ile StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3630cb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ NLTK ve Türkçe stopwords kurulu\n",
      "\n",
      "\n",
      "NLTK STOPWORDS KALDIRMA:\n",
      "==================================================\n",
      "Orijinal kelime sayısı: 27\n",
      "Filtrelenmiş kelime sayısı: 20\n",
      "Filtrelenmiş kelimeler: ['bugün', 'hava', 'güzel', 'ben', 'arkadaşlarım', 'parka', 'gittik', 'orada', 'eğlendik', 'futbol', 'oynadık', 'koştuk', 'güldük', 'gerçekten', 'harika', 'bir', 'gündü', 'yarın', 'tekrar', 'gideceğiz']\n",
      "\n",
      "NLTK Türkçe stopwords (ilk 20): ['hiç', 'ile', 'şu', 'bu', 'hep', 'nasıl', 'yani', 'neden', 'için', 'ya', 'en', 'hem', 'ki', 'veya', 'çünkü', 'biri', 'daha', 'ise', 'az', 'mı']\n",
      "Toplam NLTK stopwords sayısı: 53\n"
     ]
    }
   ],
   "source": [
    "# NLTK ile stopwords\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    \n",
    "    # Türkçe stopwords'leri indir\n",
    "    try:\n",
    "        turkish_stopwords_nltk = set(stopwords.words('turkish'))\n",
    "        nltk_available = True\n",
    "        print(\"✅ NLTK ve Türkçe stopwords kurulu\")\n",
    "    except:\n",
    "        try:\n",
    "            nltk.download('stopwords', quiet=True)\n",
    "            nltk.download('punkt', quiet=True)\n",
    "            turkish_stopwords_nltk = set(stopwords.words('turkish'))\n",
    "            nltk_available = True\n",
    "            print(\"✅ NLTK stopwords indirildi\")\n",
    "        except:\n",
    "            nltk_available = False\n",
    "            print(\"❌ NLTK stopwords indirilemedi\")\n",
    "            \n",
    "except ImportError:\n",
    "    print(\"❌ NLTK kurulu değil\")\n",
    "    nltk_available = False\n",
    "\n",
    "def remove_stopwords_nltk(text):\n",
    "    \"\"\"NLTK ile stopwords kaldırma\"\"\"\n",
    "    if not nltk_available:\n",
    "        return [], []\n",
    "    \n",
    "    # Tokenize et\n",
    "    tokens = word_tokenize(text.lower(), language='turkish')\n",
    "    \n",
    "    # Sadece alfabetik karakterleri al\n",
    "    words = [word for word in tokens if word.isalpha()]\n",
    "    \n",
    "    # Stopwords'leri kaldır\n",
    "    filtered_words = [word for word in words if word not in turkish_stopwords_nltk]\n",
    "    \n",
    "    return filtered_words, words\n",
    "\n",
    "if nltk_available:\n",
    "    filtered_nltk, original_nltk = remove_stopwords_nltk(sample_text)\n",
    "    \n",
    "    print(\"\\n\\nNLTK STOPWORDS KALDIRMA:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Orijinal kelime sayısı: {len(original_nltk)}\")\n",
    "    print(f\"Filtrelenmiş kelime sayısı: {len(filtered_nltk)}\")\n",
    "    print(f\"Filtrelenmiş kelimeler: {filtered_nltk}\")\n",
    "    \n",
    "    # NLTK'nın Türkçe stopwords listesini göster\n",
    "    print(f\"\\nNLTK Türkçe stopwords (ilk 20): {list(turkish_stopwords_nltk)[:20]}\")\n",
    "    print(f\"Toplam NLTK stopwords sayısı: {len(turkish_stopwords_nltk)}\")\n",
    "else:\n",
    "    print(\"\\nNLTK kurulu olmadığı için test edilemiyor.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefcf8d9",
   "metadata": {},
   "source": [
    "Sonuçları Karşılaştır"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a336b8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "STOPWORDS KALDIRMA SONUÇLARI KARŞILAŞTIRMA:\n",
      "============================================================\n",
      "Python (Manuel)     : 16 kelime\n",
      "Zeyrek              : 16 kelime\n",
      "TRNLP               : 16 kelime\n",
      "spaCy               : 18 kelime\n",
      "NLTK                : 20 kelime\n",
      "\n",
      "En fazla stopword kaldıran: Python (Manuel) (16 kelime kaldı)\n"
     ]
    }
   ],
   "source": [
    "# Tüm sonuçları karşılaştır\n",
    "print(\"\\n\\nSTOPWORDS KALDIRMA SONUÇLARI KARŞILAŞTIRMA:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = {\n",
    "    'Python (Manuel)': len(filtered_python) if 'filtered_python' in locals() else 0,\n",
    "    'Zeyrek': len(filtered_zeyrek) if 'filtered_zeyrek' in locals() and zeyrek_available else 0,\n",
    "    'TRNLP': len(filtered_trnlp) if 'filtered_trnlp' in locals() and trnlp_available else 0,\n",
    "    'spaCy': len(filtered_spacy) if 'filtered_spacy' in locals() and spacy_available else 0,\n",
    "    'NLTK': len(filtered_nltk) if 'filtered_nltk' in locals() and nltk_available else 0\n",
    "}\n",
    "\n",
    "for method, count in results.items():\n",
    "    print(f\"{method:<20}: {count} kelime\")\n",
    "\n",
    "# En etkili yöntemi bul\n",
    "if any(results.values()):\n",
    "    best_method = min(results.items(), key=lambda x: x[1] if x[1] > 0 else float('inf'))\n",
    "    print(f\"\\nEn fazla stopword kaldıran: {best_method[0]} ({best_method[1]} kelime kaldı)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c1cb78",
   "metadata": {},
   "source": [
    "Özel Türkçe StopWords Listesi Oluşturma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9eb3b887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Özel stopwords listesi: {'çok', 'bu', 'bir'}\n",
      "Orijinal: ['bu', 'araba', 'çok', 'hızlı', 'bir', 'araba']\n",
      "Özel stopwords ile filtrelenmiş: ['araba', 'hızlı', 'araba']\n"
     ]
    }
   ],
   "source": [
    "# Metin korpusundan otomatik stopwords çıkarma\n",
    "def create_custom_stopwords(texts, min_frequency=0.7):\n",
    "    \"\"\"Korpustaki metinlerin %70'inde geçen kelimeleri stopword yap\"\"\"\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    word_doc_count = defaultdict(int)\n",
    "    total_docs = len(texts)\n",
    "    \n",
    "    for text in texts:\n",
    "        words = set(re.findall(r'\\w+', text.lower(), re.UNICODE))\n",
    "        for word in words:\n",
    "            word_doc_count[word] += 1\n",
    "    \n",
    "    # Minimum frekansı geçen kelimeleri stopword yap\n",
    "    custom_stopwords = {\n",
    "        word for word, count in word_doc_count.items() \n",
    "        if count / total_docs >= min_frequency\n",
    "    }\n",
    "    \n",
    "    return custom_stopwords\n",
    "\n",
    "# Örnek korpus\n",
    "sample_corpus = [\n",
    "    \"Bu kitap çok güzel bir kitap\",\n",
    "    \"Bu film gerçekten çok iyi bir film\", \n",
    "    \"Bu restoran çok lezzetli bir restoran\",\n",
    "    \"Bu şarkı çok güzel bir şarkı\"\n",
    "]\n",
    "\n",
    "custom_stops = create_custom_stopwords(sample_corpus, min_frequency=0.75)\n",
    "print(f\"\\nÖzel stopwords listesi: {custom_stops}\")\n",
    "\n",
    "# Test et\n",
    "test_text = \"Bu araba çok hızlı bir araba\"\n",
    "words = re.findall(r'\\w+', test_text.lower(), re.UNICODE)\n",
    "filtered_custom = [word for word in words if word not in custom_stops]\n",
    "\n",
    "print(f\"Orijinal: {words}\")\n",
    "print(f\"Özel stopwords ile filtrelenmiş: {filtered_custom}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb8c6e0",
   "metadata": {},
   "source": [
    "\n",
    "## Pratik Öneriler\n",
    "\n",
    "### 🎯 Hangi Yöntemi Seçmeli?\n",
    "\n",
    "1. **Hızlı ve basit**: Python manuel liste\n",
    "2. **En doğru**: Zeyrek (morfolojik analiz ile)\n",
    "3. **Türkçe'ye özel**: TRNLP\n",
    "4. **Endüstriyel**: spaCy\n",
    "5. **Akademik**: NLTK\n",
    "\n",
    "### ⚡ Performans İpuçları:\n",
    "- Büyük metinler için manuel liste kullan\n",
    "- Doğruluk önemliyse morfolojik analiz yap\n",
    "- Domain-specific stopwords ekle\n",
    "- Çok kısa kelimeleri (1-2 harf) otomatik kaldır\n",
    "\n",
    "### 🔧 Özelleştirme:\n",
    "- Kendi domain'inize özel stopwords ekleyin\n",
    "- Frekans bazlı otomatik stopwords oluşturun\n",
    "- Farklı görevler için farklı listeler kullanın"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0c42e3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
