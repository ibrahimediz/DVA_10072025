{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b1d67bd",
   "metadata": {},
   "source": [
    "# TÃ¼rkÃ§e NLP: StopWords (Durak Kelimeleri)\n",
    "\n",
    "## StopWords Nedir?\n",
    "\n",
    "**StopWords (Durak Kelimeleri)**, metinlerde Ã§ok sÄ±k geÃ§en ancak genellikle anlamsal deÄŸeri dÃ¼ÅŸÃ¼k olan kelimelerdir. Ã–rneÄŸin: \"ve\", \"bir\", \"bu\", \"ÅŸu\", \"iÃ§in\", \"ile\" gibi.\n",
    "\n",
    "### Neden KaldÄ±rÄ±lÄ±r?\n",
    "- **Boyut azaltma**: Veri setinin boyutunu kÃ¼Ã§Ã¼ltÃ¼r\n",
    "- **GÃ¼rÃ¼ltÃ¼ azaltma**: Anlamsal analizi iyileÅŸtirir  \n",
    "- **Performans artÄ±ÅŸÄ±**: Ä°ÅŸlem hÄ±zÄ±nÄ± artÄ±rÄ±r\n",
    "- **Odaklanma**: Ã–nemli kelimelere odaklanmayÄ± saÄŸlar\n",
    "\n",
    "### Ne Zaman KaldÄ±rÄ±lmaz?\n",
    "- Duygu analizi (olumsuzluk ifadeleri Ã¶nemli)\n",
    "- Makine Ã§evirisi\n",
    "- Soru-cevap sistemleri\n",
    "- Metin Ã¶zetleme\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f9ef7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orijinal metin:\n",
      "\n",
      "BugÃ¼n hava Ã§ok gÃ¼zel. Ben ve arkadaÅŸlarÄ±m parka gittik.\n",
      "Orada Ã§ok eÄŸlendik. Futbol oynadÄ±k, koÅŸtuk ve Ã§ok gÃ¼ldÃ¼k.\n",
      "Bu gerÃ§ekten harika bir gÃ¼ndÃ¼. YarÄ±n da tekrar gideceÄŸiz.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"\"\"\n",
    "BugÃ¼n hava Ã§ok gÃ¼zel. Ben ve arkadaÅŸlarÄ±m parka gittik.\n",
    "Orada Ã§ok eÄŸlendik. Futbol oynadÄ±k, koÅŸtuk ve Ã§ok gÃ¼ldÃ¼k.\n",
    "Bu gerÃ§ekten harika bir gÃ¼ndÃ¼. YarÄ±n da tekrar gideceÄŸiz.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Orijinal metin:\")\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f012d52",
   "metadata": {},
   "source": [
    "## python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99d46959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYTHON STOPWORDS KALDIRMA:\n",
      "==================================================\n",
      "Orijinal kelime sayÄ±sÄ±: 27\n",
      "FiltrelenmiÅŸ kelime sayÄ±sÄ±: 16\n",
      "KaldÄ±rÄ±lan kelime sayÄ±sÄ±: 11\n",
      "\n",
      "Orijinal kelimeler: ['bugÃ¼n', 'hava', 'Ã§ok', 'gÃ¼zel', 'ben', 've', 'arkadaÅŸlarÄ±m', 'parka', 'gittik', 'orada', 'Ã§ok', 'eÄŸlendik', 'futbol', 'oynadÄ±k', 'koÅŸtuk', 've', 'Ã§ok', 'gÃ¼ldÃ¼k', 'bu', 'gerÃ§ekten', 'harika', 'bir', 'gÃ¼ndÃ¼', 'yarÄ±n', 'da', 'tekrar', 'gideceÄŸiz']\n",
      "\n",
      "FiltrelenmiÅŸ kelimeler: ['hava', 'gÃ¼zel', 'arkadaÅŸlarÄ±m', 'parka', 'gittik', 'orada', 'eÄŸlendik', 'futbol', 'oynadÄ±k', 'koÅŸtuk', 'gÃ¼ldÃ¼k', 'gerÃ§ekten', 'harika', 'gÃ¼ndÃ¼', 'tekrar', 'gideceÄŸiz']\n",
      "\n",
      "KaldÄ±rÄ±lan stopwords: ['bugÃ¼n', 'bir', 'bu', 'yarÄ±n', 'Ã§ok', 've', 'da', 'ben']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# TÃ¼rkÃ§e stopwords listesi (manuel)\n",
    "turkish_stopwords = {\n",
    "    've', 'bir', 'bu', 'ÅŸu', 'o', 'ben', 'sen', 'biz', 'siz', 'onlar',\n",
    "    'iÃ§in', 'ile', 'da', 'de', 'ta', 'te', 'den', 'dan', 'ten', 'tan',\n",
    "    'nin', 'nÄ±n', 'nun', 'nÃ¼n', 'in', 'Ä±n', 'un', 'Ã¼n',\n",
    "    'ya', 'ye', 'na', 'ne', 'a', 'e', 'Ä±', 'i', 'u', 'Ã¼', 'o', 'Ã¶',\n",
    "    'Ã§ok', 'daha', 'en', 'hem', 'hiÃ§', 'her', 'hangi', 'kendi',\n",
    "    'sonra', 'Ã¶nce', 'ÅŸimdi', 'bugÃ¼n', 'yarÄ±n', 'dÃ¼n',\n",
    "    'ama', 'fakat', 'ancak', 'lakin', 'ki', 'eÄŸer', 'ise',\n",
    "    'gibi', 'kadar', 'gÃ¶re', 'doÄŸru', 'karÅŸÄ±', 'raÄŸmen',\n",
    "    'mi', 'mÄ±', 'mu', 'mÃ¼', 'midir', 'mÄ±dÄ±r', 'mudur', 'mÃ¼dÃ¼r'\n",
    "}\n",
    "\n",
    "def remove_stopwords_python(text, stopwords):\n",
    "    \"\"\"Python ile stopwords kaldÄ±rma\"\"\"\n",
    "    # Metni kelimelere ayÄ±r\n",
    "    words = re.findall(r'\\w+', text.lower(), re.UNICODE)\n",
    "    \n",
    "    # Stopwords'leri kaldÄ±r\n",
    "    filtered_words = [word for word in words if word not in stopwords]\n",
    "    \n",
    "    return filtered_words, words\n",
    "\n",
    "# Test et\n",
    "filtered_python, original_words = remove_stopwords_python(sample_text, turkish_stopwords)\n",
    "\n",
    "print(\"PYTHON STOPWORDS KALDIRMA:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Orijinal kelime sayÄ±sÄ±: {len(original_words)}\")\n",
    "print(f\"FiltrelenmiÅŸ kelime sayÄ±sÄ±: {len(filtered_python)}\")\n",
    "print(f\"KaldÄ±rÄ±lan kelime sayÄ±sÄ±: {len(original_words) - len(filtered_python)}\")\n",
    "\n",
    "print(f\"\\nOrijinal kelimeler: {original_words}\")\n",
    "print(f\"\\nFiltrelenmiÅŸ kelimeler: {filtered_python}\")\n",
    "\n",
    "# KaldÄ±rÄ±lan kelimeleri gÃ¶ster\n",
    "removed_words = [word for word in original_words if word in turkish_stopwords]\n",
    "print(f\"\\nKaldÄ±rÄ±lan stopwords: {list(set(removed_words))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c696b54",
   "metadata": {},
   "source": [
    "2. Zeyrek ile StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d48c89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "APPENDING RESULT: <(bugÃ¼n_Adv)(-)(bugÃ¼n:advRoot_ST)>\n",
      "APPENDING RESULT: <(bugÃ¼n_Noun_Time)(-)(bugÃ¼n:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(hava_Adj)(-)(hava:adjectiveRoot_ST)>\n",
      "APPENDING RESULT: <(hav_Noun)(-)(hav:noun_S + a3sg_S + pnon_S + a:dat_ST)>\n",
      "APPENDING RESULT: <(hava_Noun)(-)(hava:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(Ã§ok_Adj)(-)(Ã§ok:adjectiveRoot_ST)>\n",
      "APPENDING RESULT: <(Ã§ok_Det)(-)(Ã§ok:detRoot_ST)>\n",
      "APPENDING RESULT: <(Ã§ok_Adv)(-)(Ã§ok:advRoot_ST)>\n",
      "APPENDING RESULT: <(Ã§ok_Postp_PCAbl)(-)(Ã§ok:postpRoot_ST)>\n",
      "APPENDING RESULT: <(gÃ¼zel_Adv)(-)(gÃ¼zel:advRoot_ST)>\n",
      "APPENDING RESULT: <(gÃ¼zel_Adj)(-)(gÃ¼zel:adjectiveRoot_ST)>\n",
      "APPENDING RESULT: <(gÃ¼zel_Noun)(-)(gÃ¼zel:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(ben_Noun)(-)(ben:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(ben_Pron_Pers)(-)(ben:pronPers_S + pA1sg_S + pPnon_S + pNom_ST)>\n",
      "APPENDING RESULT: <(ve_Conj)(-)(ve:conjRoot_ST)>\n",
      "APPENDING RESULT: <(arkadaÅŸ_Noun)(-)(arkadaÅŸ:noun_S + lar:a3pl_S + Ä±m:p1sg_S + nom_ST)>\n",
      "APPENDING RESULT: <(arkadaÅŸ_Noun)(-)(arkadaÅŸ:noun_S + lar:a3pl_S + pnon_S + nom_ST + nounZeroDeriv_S + nVerb_S + nPresent_S + Ä±m:nA1sg_ST)>\n",
      "APPENDING RESULT: <(park_Noun)(-)(park:noun_S + a3sg_S + pnon_S + a:dat_ST)>\n",
      "APPENDING RESULT: <(Park_Noun_Prop)(-)(park:nounProper_S + a3sg_S + pnon_S + a:dat_ST)>\n",
      "APPENDING RESULT: <(parka_Noun)(-)(parka:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(gitmek_Verb)(-)(git:verbRoot_S + ti:vPast_S + k:vA1pl_ST)>\n",
      "APPENDING RESULT: <(gitmek_Verb)(-)(git:verbRoot_S + tik:vPastPart_S + adjAfterVerb_S + aPnon_ST)>\n",
      "APPENDING RESULT: <(gitmek_Verb)(-)(git:verbRoot_S + tik:vPastPart_S + noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(ora_Noun)(-)(ora:noun_S + a3sg_S + pnon_S + da:loc_ST)>\n",
      "APPENDING RESULT: <(Ã§ok_Adj)(-)(Ã§ok:adjectiveRoot_ST)>\n",
      "APPENDING RESULT: <(Ã§ok_Det)(-)(Ã§ok:detRoot_ST)>\n",
      "APPENDING RESULT: <(Ã§ok_Adv)(-)(Ã§ok:advRoot_ST)>\n",
      "APPENDING RESULT: <(Ã§ok_Postp_PCAbl)(-)(Ã§ok:postpRoot_ST)>\n",
      "APPENDING RESULT: <(eÄŸlenmek_Verb)(-)(eÄŸlen:verbRoot_S + di:vPast_S + k:vA1pl_ST)>\n",
      "APPENDING RESULT: <(eÄŸlenmek_Verb)(-)(eÄŸlen:verbRoot_S + dik:vPastPart_S + adjAfterVerb_S + aPnon_ST)>\n",
      "APPENDING RESULT: <(eÄŸlemek_Verb)(-)(eÄŸle:verbRoot_S + n:vPass_S + verbRoot_S + di:vPast_S + k:vA1pl_ST)>\n",
      "APPENDING RESULT: <(eÄŸlemek_Verb)(-)(eÄŸle:verbRoot_S + n:vPass_S + verbRoot_S + dik:vPastPart_S + adjAfterVerb_S + aPnon_ST)>\n",
      "APPENDING RESULT: <(eÄŸlenmek_Verb)(-)(eÄŸlen:verbRoot_S + dik:vPastPart_S + noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(eÄŸlemek_Verb)(-)(eÄŸle:verbRoot_S + n:vPass_S + verbRoot_S + dik:vPastPart_S + noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(futbol_Noun)(-)(futbol:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(oynamak_Verb)(-)(oyna:verbRoot_S + dÄ±:vPast_S + k:vA1pl_ST)>\n",
      "APPENDING RESULT: <(oynamak_Verb)(-)(oyna:verbRoot_S + dÄ±k:vPastPart_S + adjAfterVerb_S + aPnon_ST)>\n",
      "APPENDING RESULT: <(oynamak_Verb)(-)(oyna:verbRoot_S + dÄ±k:vPastPart_S + noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(koÅŸmak_Verb)(-)(koÅŸ:verbRoot_S + tu:vPast_S + k:vA1pl_ST)>\n",
      "APPENDING RESULT: <(koÅŸmak_Verb)(-)(koÅŸ:verbRoot_S + tuk:vPastPart_S + adjAfterVerb_S + aPnon_ST)>\n",
      "APPENDING RESULT: <(koÅŸmak_Verb)(-)(koÅŸ:verbRoot_S + tuk:vPastPart_S + noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(ve_Conj)(-)(ve:conjRoot_ST)>\n",
      "APPENDING RESULT: <(Ã§ok_Adj)(-)(Ã§ok:adjectiveRoot_ST)>\n",
      "APPENDING RESULT: <(Ã§ok_Det)(-)(Ã§ok:detRoot_ST)>\n",
      "APPENDING RESULT: <(Ã§ok_Adv)(-)(Ã§ok:advRoot_ST)>\n",
      "APPENDING RESULT: <(Ã§ok_Postp_PCAbl)(-)(Ã§ok:postpRoot_ST)>\n",
      "APPENDING RESULT: <(gÃ¼lmek_Verb)(-)(gÃ¼l:verbRoot_S + dÃ¼:vPast_S + k:vA1pl_ST)>\n",
      "APPENDING RESULT: <(gÃ¼lmek_Verb)(-)(gÃ¼l:verbRoot_S + dÃ¼k:vPastPart_S + adjAfterVerb_S + aPnon_ST)>\n",
      "APPENDING RESULT: <(gÃ¼lmek_Verb)(-)(gÃ¼l:verbRoot_S + dÃ¼k:vPastPart_S + noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(gÃ¼l_Noun)(-)(gÃ¼l:noun_S + a3sg_S + pnon_S + nom_ST + nounZeroDeriv_S + nVerb_S + dÃ¼:nPast_S + k:nA1pl_ST)>\n",
      "APPENDING RESULT: <(bu_Det)(-)(bu:detRoot_ST)>\n",
      "APPENDING RESULT: <(bu_Pron_Demons)(-)(bu:pronDemons_S + pA3sg_S + pPnon_S + pNom_ST)>\n",
      "APPENDING RESULT: <(gerÃ§ekten_Adv)(-)(gerÃ§ekten:advRoot_ST)>\n",
      "APPENDING RESULT: <(GerÃ§ek_Noun_Prop)(-)(gerÃ§ek:nounProper_S + a3sg_S + pnon_S + ten:abl_ST)>\n",
      "APPENDING RESULT: <(gerÃ§ek_Noun)(-)(gerÃ§ek:noun_S + a3sg_S + pnon_S + ten:abl_ST)>\n",
      "APPENDING RESULT: <(gerÃ§ek_Adj)(-)(gerÃ§ek:adjectiveRoot_ST + adjZeroDeriv_S + noun_S + a3sg_S + pnon_S + ten:abl_ST)>\n",
      "APPENDING RESULT: <(harika_Interj)(-)(harika:interjRoot_ST)>\n",
      "APPENDING RESULT: <(harika_Adj)(-)(harika:adjectiveRoot_ST)>\n",
      "APPENDING RESULT: <(Harika_Noun_Prop)(-)(harika:nounProper_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(bir_Num_Card)(-)(bir:numeralRoot_ST)>\n",
      "APPENDING RESULT: <(bir_Det)(-)(bir:detRoot_ST)>\n",
      "APPENDING RESULT: <(bir_Adj)(-)(bir:adjectiveRoot_ST)>\n",
      "APPENDING RESULT: <(bir_Adv)(-)(bir:advRoot_ST)>\n",
      "APPENDING RESULT: <(gÃ¼n_Noun_Time)(-)(gÃ¼n:noun_S + a3sg_S + pnon_S + nom_ST + nounZeroDeriv_S + nVerb_S + dÃ¼:nPast_S + nA3sg_ST)>\n",
      "APPENDING RESULT: <(yarÄ±n_Adv)(-)(yarÄ±n:advRoot_ST)>\n",
      "APPENDING RESULT: <(yarmak_Verb)(-)(yar:verbRoot_S + vImp_S + Ä±n:vA2pl_ST)>\n",
      "APPENDING RESULT: <(yar_Noun)(-)(yar:noun_S + a3sg_S + pnon_S + Ä±n:gen_ST)>\n",
      "APPENDING RESULT: <(yar_Noun)(-)(yar:noun_S + a3sg_S + Ä±n:p2sg_S + nom_ST)>\n",
      "APPENDING RESULT: <(yarÄ±_Noun)(-)(yarÄ±:noun_S + a3sg_S + n:p2sg_S + nom_ST)>\n",
      "APPENDING RESULT: <(yarÄ±n_Noun_Time)(-)(yarÄ±n:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(yarÄ±_Adj)(-)(yarÄ±:adjectiveRoot_ST + adjZeroDeriv_S + noun_S + a3sg_S + n:p2sg_S + nom_ST)>\n",
      "APPENDING RESULT: <(da_Conj)(-)(da:conjRoot_ST)>\n",
      "APPENDING RESULT: <(tekrar_Adv)(-)(tekrar:advRoot_ST)>\n",
      "APPENDING RESULT: <(tekrar_Noun)(-)(tekrar:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(gitmek_Verb)(-)(gid:verbRoot_S + eceÄŸ:vFut_S + iz:vA1pl_ST)>\n",
      "APPENDING RESULT: <(Gide_Noun_Prop)(-)(gide:nounProper_S + a3sg_S + pnon_S + nom_ST + ceÄŸiz:dim_S + noun_S + a3sg_S + pnon_S + nom_ST)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeyrek ile stopwords kaldÄ±rÄ±lmÄ±ÅŸ kelimeler:\n",
      "['hava', 'gÃ¼zel', 'arkadaÅŸlarÄ±m', 'parka', 'gittik', 'orada', 'eÄŸlendik', 'futbol', 'oynadÄ±k', 'koÅŸtuk', 'gÃ¼ldÃ¼k', 'gerÃ§ekten', 'harika', 'gÃ¼ndÃ¼', 'tekrar', 'gideceÄŸiz']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "try:\n",
    "    import zeyrek\n",
    "    analyzer = zeyrek.MorphAnalyzer()\n",
    "    zeyrek_available = True\n",
    "except ImportError:\n",
    "    print(\"Zeyrek kurulu deÄŸil.\")\n",
    "    zeyrek_available = False\n",
    "\n",
    "def remove_stopwords_zeyrek(text, stopwords):\n",
    "    if not zeyrek_available:\n",
    "        return []\n",
    "    words = re.findall(r'\\w+', text.lower(), re.UNICODE)\n",
    "    filtered_words = []\n",
    "    for word in words:\n",
    "        analyses = analyzer.analyze(word)\n",
    "        # Analiz sonucu ve lemma listesi var mÄ± kontrolÃ¼\n",
    "        if analyses and len(analyses) > 0 and len(analyses[0]) > 1 and analyses[0][1]:\n",
    "            lemma = analyses[0][1][0]\n",
    "        else:\n",
    "            lemma = word\n",
    "        if lemma not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "    return filtered_words\n",
    "\n",
    "# Ã–rnek metin ve stopwords listesi yukarÄ±da tanÄ±mlÄ± olmalÄ±\n",
    "if zeyrek_available:\n",
    "    filtered_zeyrek = remove_stopwords_zeyrek(sample_text, turkish_stopwords)\n",
    "    print(\"Zeyrek ile stopwords kaldÄ±rÄ±lmÄ±ÅŸ kelimeler:\")\n",
    "    print(filtered_zeyrek)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89515e4",
   "metadata": {},
   "source": [
    "3. TRNLP ile StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce2640b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRNLP ile stopwords kaldÄ±rÄ±lmÄ±ÅŸ kelimeler:\n",
      "['hava', 'gÃ¼zel', 'arkadaÅŸlarÄ±m', 'parka', 'gittik', 'orada', 'eÄŸlendik', 'futbol', 'oynadÄ±k', 'koÅŸtuk', 'gÃ¼ldÃ¼k', 'gerÃ§ekten', 'harika', 'gÃ¼ndÃ¼', 'tekrar', 'gideceÄŸiz']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "try:\n",
    "    from trnlp import TrnlpWord\n",
    "    trnlp_available = True\n",
    "except ImportError:\n",
    "    print(\"TRNLP kurulu deÄŸil.\")\n",
    "    trnlp_available = False\n",
    "\n",
    "def remove_stopwords_trnlp(text, stopwords):\n",
    "    if not trnlp_available:\n",
    "        return []\n",
    "    words = re.findall(r'\\w+', text.lower(), re.UNICODE)\n",
    "    filtered_words = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            tr_word = TrnlpWord()\n",
    "            tr_word.setword(word)\n",
    "            # BazÄ± sÃ¼rÃ¼mlerde lemma, bazÄ± sÃ¼rÃ¼mlerde lemmas olabilir\n",
    "            lemma = getattr(tr_word, \"lemma\", None) or getattr(tr_word, \"lemmas\", None)\n",
    "            # lemma bazen None, bazen boÅŸ string, bazen liste olabilir\n",
    "            if isinstance(lemma, list):\n",
    "                lemma = lemma[0] if lemma else word\n",
    "            elif not lemma:\n",
    "                lemma = word\n",
    "        except Exception as e:\n",
    "            lemma = word\n",
    "        if lemma not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "    return filtered_words\n",
    "\n",
    "# Ã–rnek metin ve stopwords listesi yukarÄ±da tanÄ±mlÄ± olmalÄ±\n",
    "if trnlp_available:\n",
    "    filtered_trnlp = remove_stopwords_trnlp(sample_text, turkish_stopwords)\n",
    "    print(\"TRNLP ile stopwords kaldÄ±rÄ±lmÄ±ÅŸ kelimeler:\")\n",
    "    print(filtered_trnlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b2b9f6",
   "metadata": {},
   "source": [
    "4. spaCy ile StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8414821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… spaCy ve TÃ¼rkÃ§e modeli kurulu\n",
      "\n",
      "\n",
      "SPACY STOPWORDS KALDIRMA:\n",
      "==================================================\n",
      "FiltrelenmiÅŸ kelime sayÄ±sÄ±: 18\n",
      "FiltrelenmiÅŸ kelimeler: ['bugÃ¼n', 'hava', 'gÃ¼zel', 'arkadaÅŸlarÄ±m', 'parka', 'gittik', 'eÄŸlendik', 'futbol', 'oynadÄ±k', 'koÅŸtuk', 'gÃ¼ldÃ¼k', 'gerÃ§ekten', 'harika', 'bir', 'gÃ¼ndÃ¼', 'yarÄ±n', 'tekrar', 'gideceÄŸiz']\n",
      "\n",
      "Analiz detaylarÄ±:\n",
      "  bugÃ¼n â†’ bugÃ¼n (POS: NOUN)\n",
      "  hava â†’ hava (POS: NOUN)\n",
      "  Ã§ok â†’ Ã§ok (STOPWORD)\n",
      "  gÃ¼zel â†’ gÃ¼zel (POS: ADJ)\n",
      "  ben â†’ ben (STOPWORD)\n",
      "  ve â†’ ve (STOPWORD)\n",
      "  arkadaÅŸlarÄ±m â†’ arkadaÅŸ (POS: NOUN)\n",
      "  parka â†’ park (POS: NOUN)\n",
      "  gittik â†’ git (POS: VERB)\n",
      "  orada â†’ ora (STOPWORD)\n",
      "  Ã§ok â†’ Ã§ok (STOPWORD)\n",
      "  eÄŸlendik â†’ eÄŸlen (POS: VERB)\n",
      "  futbol â†’ futbol (POS: NOUN)\n",
      "  oynadÄ±k â†’ oyna (POS: VERB)\n",
      "  koÅŸtuk â†’ koÅŸtuk (POS: VERB)\n",
      "  ve â†’ ve (STOPWORD)\n",
      "  Ã§ok â†’ Ã§ok (STOPWORD)\n",
      "  gÃ¼ldÃ¼k â†’ gÃ¼ldÃ¼k (POS: VERB)\n",
      "  bu â†’ bu (STOPWORD)\n",
      "  gerÃ§ekten â†’ gerÃ§ekten (POS: ADV)\n",
      "  harika â†’ harika (POS: ADJ)\n",
      "  bir â†’ bir (POS: DET)\n",
      "  gÃ¼ndÃ¼ â†’ gÃ¼ndÃ¼ (POS: NOUN)\n",
      "  yarÄ±n â†’ yarÄ±n (POS: NOUN)\n",
      "  da â†’ da (STOPWORD)\n",
      "  tekrar â†’ tekrar (POS: ADV)\n",
      "  gideceÄŸiz â†’ git (POS: VERB)\n"
     ]
    }
   ],
   "source": [
    "# spaCy ile stopwords\n",
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"tr_core_news_md\")\n",
    "    spacy_available = True\n",
    "    print(\"âœ… spaCy ve TÃ¼rkÃ§e modeli kurulu\")\n",
    "except:\n",
    "    print(\"âŒ spaCy veya TÃ¼rkÃ§e modeli kurulu deÄŸil\")\n",
    "    spacy_available = False\n",
    "\n",
    "def remove_stopwords_spacy(text):\n",
    "    \"\"\"spaCy ile stopwords kaldÄ±rma (kendi stopwords listesi)\"\"\"\n",
    "    if not spacy_available:\n",
    "        return [], []\n",
    "    \n",
    "    doc = nlp(text.lower())\n",
    "    \n",
    "    filtered_words = []\n",
    "    analysis_info = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.is_alpha:  # Sadece alfabetik karakterler\n",
    "            if not token.is_stop:  # spaCy'nin stopword listesi\n",
    "                filtered_words.append(token.text)\n",
    "                analysis_info.append(f\"{token.text} â†’ {token.lemma_} (POS: {token.pos_})\")\n",
    "            else:\n",
    "                analysis_info.append(f\"{token.text} â†’ {token.lemma_} (STOPWORD)\")\n",
    "    \n",
    "    return filtered_words, analysis_info\n",
    "\n",
    "if spacy_available:\n",
    "    filtered_spacy, spacy_analysis = remove_stopwords_spacy(sample_text)\n",
    "    \n",
    "    print(\"\\n\\nSPACY STOPWORDS KALDIRMA:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"FiltrelenmiÅŸ kelime sayÄ±sÄ±: {len(filtered_spacy)}\")\n",
    "    print(f\"FiltrelenmiÅŸ kelimeler: {filtered_spacy}\")\n",
    "    \n",
    "    print(f\"\\nAnaliz detaylarÄ±:\")\n",
    "    for analysis in spacy_analysis:\n",
    "        print(f\"  {analysis}\")\n",
    "else:\n",
    "    print(\"\\nspaCy kurulu olmadÄ±ÄŸÄ± iÃ§in test edilemiyor.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e521d4bd",
   "metadata": {},
   "source": [
    "5. NLTK ile StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3630cb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… NLTK ve TÃ¼rkÃ§e stopwords kurulu\n",
      "\n",
      "\n",
      "NLTK STOPWORDS KALDIRMA:\n",
      "==================================================\n",
      "Orijinal kelime sayÄ±sÄ±: 27\n",
      "FiltrelenmiÅŸ kelime sayÄ±sÄ±: 20\n",
      "FiltrelenmiÅŸ kelimeler: ['bugÃ¼n', 'hava', 'gÃ¼zel', 'ben', 'arkadaÅŸlarÄ±m', 'parka', 'gittik', 'orada', 'eÄŸlendik', 'futbol', 'oynadÄ±k', 'koÅŸtuk', 'gÃ¼ldÃ¼k', 'gerÃ§ekten', 'harika', 'bir', 'gÃ¼ndÃ¼', 'yarÄ±n', 'tekrar', 'gideceÄŸiz']\n",
      "\n",
      "NLTK TÃ¼rkÃ§e stopwords (ilk 20): ['hiÃ§', 'ile', 'ÅŸu', 'bu', 'hep', 'nasÄ±l', 'yani', 'neden', 'iÃ§in', 'ya', 'en', 'hem', 'ki', 'veya', 'Ã§Ã¼nkÃ¼', 'biri', 'daha', 'ise', 'az', 'mÄ±']\n",
      "Toplam NLTK stopwords sayÄ±sÄ±: 53\n"
     ]
    }
   ],
   "source": [
    "# NLTK ile stopwords\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    \n",
    "    # TÃ¼rkÃ§e stopwords'leri indir\n",
    "    try:\n",
    "        turkish_stopwords_nltk = set(stopwords.words('turkish'))\n",
    "        nltk_available = True\n",
    "        print(\"âœ… NLTK ve TÃ¼rkÃ§e stopwords kurulu\")\n",
    "    except:\n",
    "        try:\n",
    "            nltk.download('stopwords', quiet=True)\n",
    "            nltk.download('punkt', quiet=True)\n",
    "            turkish_stopwords_nltk = set(stopwords.words('turkish'))\n",
    "            nltk_available = True\n",
    "            print(\"âœ… NLTK stopwords indirildi\")\n",
    "        except:\n",
    "            nltk_available = False\n",
    "            print(\"âŒ NLTK stopwords indirilemedi\")\n",
    "            \n",
    "except ImportError:\n",
    "    print(\"âŒ NLTK kurulu deÄŸil\")\n",
    "    nltk_available = False\n",
    "\n",
    "def remove_stopwords_nltk(text):\n",
    "    \"\"\"NLTK ile stopwords kaldÄ±rma\"\"\"\n",
    "    if not nltk_available:\n",
    "        return [], []\n",
    "    \n",
    "    # Tokenize et\n",
    "    tokens = word_tokenize(text.lower(), language='turkish')\n",
    "    \n",
    "    # Sadece alfabetik karakterleri al\n",
    "    words = [word for word in tokens if word.isalpha()]\n",
    "    \n",
    "    # Stopwords'leri kaldÄ±r\n",
    "    filtered_words = [word for word in words if word not in turkish_stopwords_nltk]\n",
    "    \n",
    "    return filtered_words, words\n",
    "\n",
    "if nltk_available:\n",
    "    filtered_nltk, original_nltk = remove_stopwords_nltk(sample_text)\n",
    "    \n",
    "    print(\"\\n\\nNLTK STOPWORDS KALDIRMA:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Orijinal kelime sayÄ±sÄ±: {len(original_nltk)}\")\n",
    "    print(f\"FiltrelenmiÅŸ kelime sayÄ±sÄ±: {len(filtered_nltk)}\")\n",
    "    print(f\"FiltrelenmiÅŸ kelimeler: {filtered_nltk}\")\n",
    "    \n",
    "    # NLTK'nÄ±n TÃ¼rkÃ§e stopwords listesini gÃ¶ster\n",
    "    print(f\"\\nNLTK TÃ¼rkÃ§e stopwords (ilk 20): {list(turkish_stopwords_nltk)[:20]}\")\n",
    "    print(f\"Toplam NLTK stopwords sayÄ±sÄ±: {len(turkish_stopwords_nltk)}\")\n",
    "else:\n",
    "    print(\"\\nNLTK kurulu olmadÄ±ÄŸÄ± iÃ§in test edilemiyor.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefcf8d9",
   "metadata": {},
   "source": [
    "SonuÃ§larÄ± KarÅŸÄ±laÅŸtÄ±r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a336b8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "STOPWORDS KALDIRMA SONUÃ‡LARI KARÅILAÅTIRMA:\n",
      "============================================================\n",
      "Python (Manuel)     : 16 kelime\n",
      "Zeyrek              : 16 kelime\n",
      "TRNLP               : 16 kelime\n",
      "spaCy               : 18 kelime\n",
      "NLTK                : 20 kelime\n",
      "\n",
      "En fazla stopword kaldÄ±ran: Python (Manuel) (16 kelime kaldÄ±)\n"
     ]
    }
   ],
   "source": [
    "# TÃ¼m sonuÃ§larÄ± karÅŸÄ±laÅŸtÄ±r\n",
    "print(\"\\n\\nSTOPWORDS KALDIRMA SONUÃ‡LARI KARÅILAÅTIRMA:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = {\n",
    "    'Python (Manuel)': len(filtered_python) if 'filtered_python' in locals() else 0,\n",
    "    'Zeyrek': len(filtered_zeyrek) if 'filtered_zeyrek' in locals() and zeyrek_available else 0,\n",
    "    'TRNLP': len(filtered_trnlp) if 'filtered_trnlp' in locals() and trnlp_available else 0,\n",
    "    'spaCy': len(filtered_spacy) if 'filtered_spacy' in locals() and spacy_available else 0,\n",
    "    'NLTK': len(filtered_nltk) if 'filtered_nltk' in locals() and nltk_available else 0\n",
    "}\n",
    "\n",
    "for method, count in results.items():\n",
    "    print(f\"{method:<20}: {count} kelime\")\n",
    "\n",
    "# En etkili yÃ¶ntemi bul\n",
    "if any(results.values()):\n",
    "    best_method = min(results.items(), key=lambda x: x[1] if x[1] > 0 else float('inf'))\n",
    "    print(f\"\\nEn fazla stopword kaldÄ±ran: {best_method[0]} ({best_method[1]} kelime kaldÄ±)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c1cb78",
   "metadata": {},
   "source": [
    "Ã–zel TÃ¼rkÃ§e StopWords Listesi OluÅŸturma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9eb3b887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ã–zel stopwords listesi: {'Ã§ok', 'bu', 'bir'}\n",
      "Orijinal: ['bu', 'araba', 'Ã§ok', 'hÄ±zlÄ±', 'bir', 'araba']\n",
      "Ã–zel stopwords ile filtrelenmiÅŸ: ['araba', 'hÄ±zlÄ±', 'araba']\n"
     ]
    }
   ],
   "source": [
    "# Metin korpusundan otomatik stopwords Ã§Ä±karma\n",
    "def create_custom_stopwords(texts, min_frequency=0.7):\n",
    "    \"\"\"Korpustaki metinlerin %70'inde geÃ§en kelimeleri stopword yap\"\"\"\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    word_doc_count = defaultdict(int)\n",
    "    total_docs = len(texts)\n",
    "    \n",
    "    for text in texts:\n",
    "        words = set(re.findall(r'\\w+', text.lower(), re.UNICODE))\n",
    "        for word in words:\n",
    "            word_doc_count[word] += 1\n",
    "    \n",
    "    # Minimum frekansÄ± geÃ§en kelimeleri stopword yap\n",
    "    custom_stopwords = {\n",
    "        word for word, count in word_doc_count.items() \n",
    "        if count / total_docs >= min_frequency\n",
    "    }\n",
    "    \n",
    "    return custom_stopwords\n",
    "\n",
    "# Ã–rnek korpus\n",
    "sample_corpus = [\n",
    "    \"Bu kitap Ã§ok gÃ¼zel bir kitap\",\n",
    "    \"Bu film gerÃ§ekten Ã§ok iyi bir film\", \n",
    "    \"Bu restoran Ã§ok lezzetli bir restoran\",\n",
    "    \"Bu ÅŸarkÄ± Ã§ok gÃ¼zel bir ÅŸarkÄ±\"\n",
    "]\n",
    "\n",
    "custom_stops = create_custom_stopwords(sample_corpus, min_frequency=0.75)\n",
    "print(f\"\\nÃ–zel stopwords listesi: {custom_stops}\")\n",
    "\n",
    "# Test et\n",
    "test_text = \"Bu araba Ã§ok hÄ±zlÄ± bir araba\"\n",
    "words = re.findall(r'\\w+', test_text.lower(), re.UNICODE)\n",
    "filtered_custom = [word for word in words if word not in custom_stops]\n",
    "\n",
    "print(f\"Orijinal: {words}\")\n",
    "print(f\"Ã–zel stopwords ile filtrelenmiÅŸ: {filtered_custom}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb8c6e0",
   "metadata": {},
   "source": [
    "\n",
    "## Pratik Ã–neriler\n",
    "\n",
    "### ğŸ¯ Hangi YÃ¶ntemi SeÃ§meli?\n",
    "\n",
    "1. **HÄ±zlÄ± ve basit**: Python manuel liste\n",
    "2. **En doÄŸru**: Zeyrek (morfolojik analiz ile)\n",
    "3. **TÃ¼rkÃ§e'ye Ã¶zel**: TRNLP\n",
    "4. **EndÃ¼striyel**: spaCy\n",
    "5. **Akademik**: NLTK\n",
    "\n",
    "### âš¡ Performans Ä°puÃ§larÄ±:\n",
    "- BÃ¼yÃ¼k metinler iÃ§in manuel liste kullan\n",
    "- DoÄŸruluk Ã¶nemliyse morfolojik analiz yap\n",
    "- Domain-specific stopwords ekle\n",
    "- Ã‡ok kÄ±sa kelimeleri (1-2 harf) otomatik kaldÄ±r\n",
    "\n",
    "### ğŸ”§ Ã–zelleÅŸtirme:\n",
    "- Kendi domain'inize Ã¶zel stopwords ekleyin\n",
    "- Frekans bazlÄ± otomatik stopwords oluÅŸturun\n",
    "- FarklÄ± gÃ¶revler iÃ§in farklÄ± listeler kullanÄ±n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0c42e3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
