{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19400539",
   "metadata": {},
   "source": [
    "# Veri Temizleme\n",
    "1. Boşlukların Temizlenmesi\n",
    "2. Büyük küçük harf dönüşümü\n",
    "3. Noktalama işaretlerinin kaldırılması\n",
    "4. Özel Karakterlerin Kaldırılması\n",
    "5. Yazım hatalarının düzeltilmesi\n",
    "6. HTML ve URL temizleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b71ad3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORİJİNAL METİN:\n",
      "'   Merhaba DÜNYA!!! Bu bir test metnidir... \\nWeb sitesi: https://www.example.com \\nE-posta: test@email.com\\nHTML: <p>Bu bir paragraf</p>\\nÖzel karakterler: @#$%^&*()\\nYazım hatası: mrhaba, dnya, kitaplarında   '\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Örnek metin - Türkçe veri temizleme için test metni\n",
    "sample_text = \"\"\"   Merhaba DÜNYA!!! Bu bir test metnidir... \n",
    "Web sitesi: https://www.example.com \n",
    "E-posta: test@email.com\n",
    "HTML: <p>Bu bir paragraf</p>\n",
    "Özel karakterler: @#$%^&*()\n",
    "Yazım hatası: mrhaba, dnya, kitaplarında   \"\"\"\n",
    "print(\"ORİJİNAL METİN:\")\n",
    "print(repr(sample_text))\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273b97e2",
   "metadata": {},
   "source": [
    "## Python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35553b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Temizleme Sonucu:\n",
      "merhaba dünya bu bir test metnidir web sitesi eposta html bu bir paragraf özel karakterler yazım hatası mrhaba dnya kitaplarında\n",
      "\n",
      "Yazım Düzeltmeli:\n",
      "merhaba dünya bu bir test metnidir web sitesi eposta html bu bir paragraf özel karakterler yazım hatası merhaba dünya kitap\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string \n",
    "import html\n",
    "\n",
    "def python_clean_text(text):\n",
    "    \"\"\"Python standart kütüphaneleri ile metin temizleme\"\"\"\n",
    "    # 1. Boşlukların temizlenmesi\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Çoklu boşlukları tek boşluğa çevir\n",
    "    \n",
    "    # 2. Büyük küçük harf dönüşümü\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 3. HTML temizleme\n",
    "    text = html.unescape(text)  # HTML entities\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # HTML tags\n",
    "    \n",
    "    # 4. URL temizleme\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    text = re.sub(r'www\\.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    \n",
    "    # 5. E-posta temizleme\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # 6. Noktalama işaretlerinin kaldırılması\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # 7. Özel karakterlerin kaldırılması\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # 8. Sayıların kaldırılması (isteğe bağlı)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # 9. Fazla boşlukları temizle\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# Yazım hatası düzeltme için basit bir sözlük\n",
    "spelling_corrections = {\n",
    "    'mrhaba': 'merhaba',\n",
    "    'dnya': 'dünya',\n",
    "    'kitaplarında': 'kitap'\n",
    "}\n",
    "\n",
    "def correct_spelling_python(text):\n",
    "    \"\"\"Basit yazım hatası düzeltme\"\"\"\n",
    "    words = text.split()\n",
    "    corrected_words = []\n",
    "    for word in words:\n",
    "        if word in spelling_corrections:\n",
    "            corrected_words.append(spelling_corrections[word])\n",
    "        else:\n",
    "            corrected_words.append(word)\n",
    "    return ' '.join(corrected_words)\n",
    "python_result = python_clean_text(sample_text)\n",
    "print(\"Python Temizleme Sonucu:\")\n",
    "print(python_result)\n",
    "\n",
    "python_corrected = correct_spelling_python(python_result)\n",
    "print(\"\\nYazım Düzeltmeli:\")\n",
    "print(python_corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dfa45f",
   "metadata": {},
   "source": [
    "## Zeyrek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e77bf0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "APPENDING RESULT: <(merhaba_Interj)(-)(merhaba:interjRoot_ST)>\n",
      "APPENDING RESULT: <(merhaba_Noun)(-)(merhaba:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(dünya_Noun)(-)(dünya:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(Dünya_Noun_Prop)(-)(dünya:nounProper_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(bu_Det)(-)(bu:detRoot_ST)>\n",
      "APPENDING RESULT: <(bu_Pron_Demons)(-)(bu:pronDemons_S + pA3sg_S + pPnon_S + pNom_ST)>\n",
      "APPENDING RESULT: <(bir_Adj)(-)(bir:adjectiveRoot_ST)>\n",
      "APPENDING RESULT: <(bir_Det)(-)(bir:detRoot_ST)>\n",
      "APPENDING RESULT: <(bir_Adv)(-)(bir:advRoot_ST)>\n",
      "APPENDING RESULT: <(bir_Num_Card)(-)(bir:numeralRoot_ST)>\n",
      "APPENDING RESULT: <(test_Noun)(-)(test:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(metin_Noun)(-)(metn:noun_S + a3sg_S + i:p3sg_S + nom_ST + nounZeroDeriv_S + nVerb_S + nPresent_S + nA3sg_S + dir:nCop_ST)>\n",
      "APPENDING RESULT: <(Web_Noun_Abbrv)(-)(web:nounAbbrv_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(site_Noun)(-)(site:noun_S + a3sg_S + si:p3sg_S + nom_ST)>\n",
      "APPENDING RESULT: <(e-posta_Noun)(-)(eposta:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(Html_Noun_Abbrv)(-)(html:nounAbbrv_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(bu_Det)(-)(bu:detRoot_ST)>\n",
      "APPENDING RESULT: <(bu_Pron_Demons)(-)(bu:pronDemons_S + pA3sg_S + pPnon_S + pNom_ST)>\n",
      "APPENDING RESULT: <(bir_Adj)(-)(bir:adjectiveRoot_ST)>\n",
      "APPENDING RESULT: <(bir_Det)(-)(bir:detRoot_ST)>\n",
      "APPENDING RESULT: <(bir_Adv)(-)(bir:advRoot_ST)>\n",
      "APPENDING RESULT: <(bir_Num_Card)(-)(bir:numeralRoot_ST)>\n",
      "APPENDING RESULT: <(paragraf_Noun)(-)(paragraf:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(özel_Adj)(-)(özel:adjectiveRoot_ST)>\n",
      "APPENDING RESULT: <(karakter_Noun)(-)(karakter:noun_S + ler:a3pl_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(karakter_Noun)(-)(karakter:noun_S + a3sg_S + pnon_S + nom_ST + nounZeroDeriv_S + nVerb_S + nPresent_S + ler:nA3pl_ST)>\n",
      "APPENDING RESULT: <(yaz_Noun_Time)(-)(yaz:noun_S + a3sg_S + ım:p1sg_S + nom_ST)>\n",
      "APPENDING RESULT: <(yazı_Noun)(-)(yazı:noun_S + a3sg_S + m:p1sg_S + nom_ST)>\n",
      "APPENDING RESULT: <(yazım_Noun)(-)(yazım:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(yaz_Noun_Time)(-)(yaz:noun_S + a3sg_S + pnon_S + nom_ST + nounZeroDeriv_S + nVerb_S + nPresent_S + ım:nA1sg_ST)>\n",
      "APPENDING RESULT: <(hata_Noun)(-)(hata:noun_S + a3sg_S + sı:p3sg_S + nom_ST)>\n",
      "APPENDING RESULT: <(kitap_Noun)(-)(kitap:noun_S + a3sg_S + ları:p3pl_S + nda:loc_ST)>\n",
      "APPENDING RESULT: <(kitap_Noun)(-)(kitap:noun_S + lar:a3pl_S + ın:p2sg_S + da:loc_ST)>\n",
      "APPENDING RESULT: <(kitap_Noun)(-)(kitap:noun_S + lar:a3pl_S + ı:p3sg_S + nda:loc_ST)>\n",
      "APPENDING RESULT: <(kitap_Noun)(-)(kitap:noun_S + lar:a3pl_S + ı:p3pl_S + nda:loc_ST)>\n",
      "APPENDING RESULT: <(Kitap_Noun_Prop)(-)(kitap:nounProper_S + a3sg_S + ları:p3pl_S + nda:loc_ST)>\n",
      "APPENDING RESULT: <(Kitap_Noun_Prop)(-)(kitap:nounProper_S + lar:a3pl_S + ın:p2sg_S + da:loc_ST)>\n",
      "APPENDING RESULT: <(Kitap_Noun_Prop)(-)(kitap:nounProper_S + lar:a3pl_S + ı:p3sg_S + nda:loc_ST)>\n",
      "APPENDING RESULT: <(Kitap_Noun_Prop)(-)(kitap:nounProper_S + lar:a3pl_S + ı:p3pl_S + nda:loc_ST)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeyrek Temizleme Sonucu:\n",
      "merhaba dünya bu bir test metnidir web sitesi eposta html bu bir paragraf özel karakterler yazım hatası mrhaba dnya kitaplarında\n"
     ]
    }
   ],
   "source": [
    "# Zeyrek kurulumu: pip install zeyrek\n",
    "try:\n",
    "    import zeyrek\n",
    "    zeyrek_available = True\n",
    "except ImportError:\n",
    "    print(\"Zeyrek kurulu değil. Kurulum: pip install zeyrek\")\n",
    "    zeyrek_available = False\n",
    "def zeyrek_clean_text(text):\n",
    "    \"\"\"Zeyrek ile metin temizleme ve morfolojik analiz\"\"\"\n",
    "    if not zeyrek_available:\n",
    "        return \"Zeyrek kurulu değil!\"\n",
    "    \n",
    "    # Temel temizleme (Python ile aynı)\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.lower()\n",
    "    text = html.unescape(text)\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    text = re.sub(r'www\\.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Zeyrek ile morfolojik analiz ve lemmatizasyon\n",
    "    analyzer = zeyrek.MorphAnalyzer()\n",
    "    words = text.split()\n",
    "    lemmatized_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word.strip():\n",
    "            try:\n",
    "                analyses = analyzer.analyze(word)\n",
    "                if analyses:\n",
    "                    # İlk analizin lemmasını al\n",
    "                    lemma = analyses[0][1][0]  # [0] = ilk analiz, [1] = lemma listesi, [0] = ilk lemma\n",
    "                    lemmatized_words.append(lemma)\n",
    "                else:\n",
    "                    lemmatized_words.append(word)\n",
    "            except:\n",
    "                lemmatized_words.append(word)\n",
    "    \n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Test et\n",
    "if zeyrek_available:\n",
    "    zeyrek_result = zeyrek_clean_text(sample_text)\n",
    "    print(\"Zeyrek Temizleme Sonucu:\")\n",
    "    print(zeyrek_result)\n",
    "else:\n",
    "    print(\"Zeyrek örnek kodu (kurulum gerekli):\")\n",
    "    print(\"zeyrek_result = zeyrek_clean_text(sample_text)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b4e662",
   "metadata": {},
   "source": [
    "## TRNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d4d9aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRNLP Temizleme Sonucu:\n",
      "merhaba dünya bu bir test metin web site eposta html bu bir paragraf özel karakter yazım hata mrhaba dnya kitap\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from trnlp import TrnlpWord\n",
    "    trnlp_available = True\n",
    "except ImportError:\n",
    "    print(\"TRNLP kurulu değil. Kurulum: pip install trnlp\")\n",
    "    trnlp_available = False\n",
    "\n",
    "def trnlp_clean_text(text):\n",
    "    \"\"\"TRNLP ile metin temizleme ve kök bulma\"\"\"\n",
    "    if not trnlp_available:\n",
    "        return \"TRNLP kurulu değil!\"\n",
    "    \n",
    "    # Temel temizleme\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.lower()\n",
    "    text = html.unescape(text)\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    text = re.sub(r'www\\.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # TRNLP ile kök bulma ve lemmatizasyon\n",
    "    words = text.split()\n",
    "    processed_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word.strip():\n",
    "            try:\n",
    "                trnlp_word = TrnlpWord()\n",
    "                trnlp_word.setword(word)\n",
    "                \n",
    "                # Kök bulma\n",
    "                stem = trnlp_word.get_stem\n",
    "                if stem and stem != word:\n",
    "                    processed_words.append(stem)\n",
    "                else:\n",
    "                    # Lemma bulma\n",
    "                    lemma = trnlp_word.get_lemma\n",
    "                    if lemma:\n",
    "                        processed_words.append(lemma)\n",
    "                    else:\n",
    "                        processed_words.append(word)\n",
    "            except:\n",
    "                processed_words.append(word)\n",
    "    \n",
    "    return ' '.join(processed_words)\n",
    "\n",
    "# Test et\n",
    "if trnlp_available:\n",
    "    trnlp_result = trnlp_clean_text(sample_text)\n",
    "    print(\"TRNLP Temizleme Sonucu:\")\n",
    "    print(trnlp_result)\n",
    "else:\n",
    "    print(\"TRNLP örnek kodu (kurulum gerekli):\")\n",
    "    print(\"trnlp_result = trnlp_clean_text(sample_text)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f592f6d",
   "metadata": {},
   "source": [
    "## spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92df5de2",
   "metadata": {},
   "source": [
    "* https://huggingface.co/turkish-nlp-suite\n",
    "* https://huggingface.co/turkish-nlp-suite/tr_core_news_md/blob/main/tr_core_news_md-1.0-py3-none-any.whl\n",
    "* https://huggingface.co/turkish-nlp-suite/tr_core_news_lg/blob/main/tr_core_news_lg-1.0-py3-none-any.whl\n",
    "* https://huggingface.co/turkish-nlp-suite/tr_core_news_trf/blob/main/tr_core_news_trf-1.0-py3-none-any.whl\n",
    "* https://github.com/turkish-nlp-suite/turkish-spacy-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dbc84ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import spacy\n",
    "    spacy_available = True\n",
    "    try:\n",
    "        nlp = spacy.load(\"tr_core_news_md\")\n",
    "        turkish_model_available = True\n",
    "    except OSError:\n",
    "        turkish_model_available = False\n",
    "        print(\"Türkçe spaCy modeli bulunamadı. Kurulum: python -m spacy download tr_core_news_sm\")\n",
    "except ImportError:\n",
    "    print(\"spaCy kurulu değil. Kurulum: pip install spacy\")\n",
    "    spacy_available = False\n",
    "    turkish_model_available = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73723d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dvapython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
