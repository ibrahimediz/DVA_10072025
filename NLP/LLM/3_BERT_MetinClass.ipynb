{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb399757",
   "metadata": {},
   "source": [
    "### 3. BERT ile Metin Sınıflandırma Uygulamaları\n",
    "\n",
    "### BERT Nedir?\n",
    "\n",
    "**BERT** (**B**idirectional **E**ncoder **R**epresentations from **T**ransformers), Google tarafından 2018’de tanıtılan, doğal dil işlemede devrim yaratan bir dil modelidir. Temel farkı, cümlenin hem soldan sağa hem de sağdan sola okunarak bağlamını anlamasıdır (çift yönlü – bidirectional).\n",
    "\n",
    "Bu sayede:\n",
    "\n",
    "- \"Banka gitmek\" ile \"Banka çöktü\" cümlelerinde \"banka\" kelimesinin farklı anlamlarını doğru ayırt edebilir.\n",
    "- Daha zengin kelime temsilleri (embeddings) üretir.\n",
    "- İleri düzey NLP görevlerinde (sınıflandırma, soru-cevap, NER) insan seviyesine yakın performans gösterir.\n",
    "\n",
    "### Neden BERT?\n",
    "\n",
    "- **Derin bidirectional training**: Kelimeler arasındaki tüm bağlamsal ilişkileri modelleyebilir.\n",
    "- **Ön eğitim + Fine-tuning**: Büyük metinlerde (Wikipedia, kitaplar) önceden eğitilir, sonra küçük veriyle hedef göreve uyarlanır.\n",
    "- **Çoklu görevde kullanılabilir**: Sınıflandırma, metin eşleştirme, metin üretimi (dolaylı), soru-cevap vs.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45209aa3",
   "metadata": {},
   "source": [
    "--------\n",
    "code\n",
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b633b345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yorum: Hızlı kargo,kaliteli ürün\n",
      "negative: 0.00688230711966753\n",
      "positive: 0.9931176900863647\n",
      "Yorum: Kalitesiz ürün iade ettim\n",
      "negative: 0.9992490410804749\n",
      "positive: 0.0007509849965572357\n",
      "Yorum: iyi ama fiyatına göre beklediğim gibi değil\n",
      "negative: 0.9981688261032104\n",
      "positive: 0.0018311868188902736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ibrahimediz/anaconda3/envs/nlp2/lib/python3.11/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "/Users/ibrahimediz/anaconda3/envs/nlp2/lib/python3.11/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\",model=\"savasy/bert-base-turkish-sentiment-cased\",return_all_scores=True)\n",
    "\n",
    "yorumlar = [\n",
    "    \"Hızlı kargo,kaliteli ürün\",\n",
    "    \"Kalitesiz ürün iade ettim\",\n",
    "    \"iyi ama fiyatına göre beklediğim gibi değil\"\n",
    "]\n",
    "\n",
    "sonuclar = classifier(yorumlar)\n",
    "\n",
    "for yorum,sonuc in zip(yorumlar,sonuclar):\n",
    "    print(\"Yorum:\",yorum)\n",
    "    for skor in sonuc:\n",
    "        etiket = skor[\"label\"]\n",
    "        puan = skor[\"score\"]\n",
    "        print(f\"{etiket}: {puan}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddec102",
   "metadata": {},
   "source": [
    "### BERT ile Sınıflandırma Nasıl Çalışır?\n",
    "\n",
    "1. **Tokenization**: Metin, alt kelimelere (subword) bölünür (WordPiece).\n",
    "2. **Embedding**: Her token’a bir vektör atanır (kelime + pozisyon + segment bilgisi).\n",
    "3. **Encoder Katmanları**: 12 veya 24 katmanlı Transformer encoder’lar bağlamı öğrenir.\n",
    "4. **[CLS] Tokenı**: Cümlenin başına eklenen özel token, tüm cümlenin özetini taşır.\n",
    "5. **Sınıflandırıcı**: `[CLS]` tokenının çıktısı, bir fully-connected katmana verilir ve sınıflar tahmin edilir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae5fa5c",
   "metadata": {},
   "source": [
    "\"BERT Text Classification Architecture\" – [CLS] + Token Embeddings + Segment + Position → Transformer Encoder → Classifier Head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b17c130",
   "metadata": {},
   "source": [
    "BERT\n",
    "\n",
    "2018’de Google tarafından geliştirildi.\n",
    "\n",
    "Çift yönlü bağlam anlayışı için Maskeli Dil Modellemesi (MLM) ve Sonraki Cümle Tahmini (NSP) görevleriyle eğitildi.\n",
    "\n",
    "Maskeli kelime tahminlerinde maskeleri sabit uygular ve NSP görevine dayanır.\n",
    "\n",
    "RoBERTa (Robustly Optimized BERT Approach)\n",
    "\n",
    "BERT mimarisini aynen kullanır ancak eğitimi iyileştirir.\n",
    "\n",
    "NSP görevini kaldırır, uzun süre ve çok daha büyük veri setleri üzerinde (16GB’den 160GB’a) eğitim yapar.\n",
    "\n",
    "Maskeleri dinamik olarak değiştirir, batch boyutunu artırır ve kelime dağarcığını büyütür.\n",
    "\n",
    "Bu iyileştirmeler performansı önemli ölçüde artırır.\n",
    "\n",
    "Örneğin GLUE, SQuAD gibi benchmarklarda BERT'ten üstün performans gösterir.\n",
    "\n",
    "DeBERTa (Decoding-enhanced BERT with Disentangled Attention)\n",
    "\n",
    "BERT’in mimarisine önemli yenilikler getirir.\n",
    "\n",
    "Dikkat mekanizmasında kelime içeriği ve konum bilgisini ayrı ayrı işler (disentangled attention).\n",
    "\n",
    "Maskeli dil modellemesinde hem mutlak hem göreli pozisyon bilgisini kullanır (enhanced mask decoder).\n",
    "\n",
    "Bu sayede bağlam ve pozisyon ilişkisini daha hassas yakalar ve karmaşık görevlerde (örneğin soru yanıtlama, semantik analiz) daha başarılı olur.\n",
    "\n",
    "Daha az veriyle bile RoBERTa ve BERT'e göre daha iyi sonuçlar elde eder.\n",
    "\n",
    "Ancak DeBERTa, RoBERTa ve BERT'e kıyasla daha fazla hesaplama ve bellek gerektirir.\n",
    "\n",
    "Özetle, RoBERTa BERT’in eğitim sürecini optimize ederek performans artışı sağlarken, DeBERTa mimari yeniliklerle konum ve bağlam bilgisini daha etkili temsil eder. Uygulamaya ve kaynaklara bağlı olarak, BERT basitlik; RoBERTa dengeli performans; DeBERTa ise yüksek doğruluk ve karmaşık görevler için tercih edilir.\n",
    "\n",
    "başka var mı böyle modeller\n",
    "başka var mı böyle modeller\n",
    "BERT, RoBERTa ve DeBERTa dışında da doğal dil işleme (NLP) için geliştirilmiş birçok benzer transformer tabanlı ön eğitimli modeller bulunmaktadır. Bunlardan bazıları şunlardır:\n",
    "\n",
    "DistilBERT: BERT'in daha küçük, daha hızlı ve hafif bir versiyonudur. Parametre sayısı BERT'in yaklaşık %40'ı kadar olup, %60 daha hızlı çalışabilir. BERT performansına yakın sonuçlar verir ancak daha az kaynak tüketir. (3)\n",
    "\n",
    "ALBERT (A Lite BERT): BERT'in daha az parametre ile benzer performansı korumayı amaçlayan modelidir. Parametre paylaşımı ve faktörizasyon teknikleri kullanarak modelin boyutunu küçültür ve eğitimini hızlandırır.\n",
    "\n",
    "XLNet: BERT'e alternatif olarak geliştirilmiş, çift yönlü bağlam anlayışı sağlarken, otoregresif bir şekilde kelime sırasını da dikkate alan bir modeldir. NSP gibi görevleri kullanmaz ama daha karmaşık eğitim stratejileriyle yüksek doğruluk sağlar.\n",
    "\n",
    "BART: Encoder-decoder yapısına sahip, hem ön eğitimde hem de sonrasında çeşitli NLP görevlerinde çok başarılı olan bir modeldir. BERT ve GPT'nin birleştirilmiş özelliklerini taşır.\n",
    "\n",
    "Electra: Daha verimli eğitim sağlayan, \"discriminative\" bir yaklaşım kullanan modeldir. Maskelenmiş tokenların yerine konması tahmini yerine gerçek ve sahte tokenları ayırt etmeye çalışır, böylece daha hızlı ve düşük maliyetli eğitim yapar.\n",
    "\n",
    "BERTweet: Twitter verileri üzerine özel ön eğitim yapılmış, sosyal medya metinleri için optimize edilmiş BERT tabanlı modeli.\n",
    "\n",
    "Bu modeller, BERT’in temel mimarisi üzerine ya da aynı transformer prensipleriyle geliştirilmiş farklı mimari değişiklikler ve eğitim stratejileri kullanılarak oluşturulmuştur. Uygulamaya göre veri boyutu, model büyüklüğü, görev karmaşıklığı ve kaynak kısıtlamalarına göre tercih edilirler.\n",
    "\n",
    "Özetle, BERT, RoBERTa, DeBERTa dışında özellikle DistilBERT, ALBERT, XLNet, BART, Electra ve BERTweet gibi modeller de yaygın olarak kullanılmaktadır ve benzer ancak spesifik avantajları olan alternatiflerdir.\n",
    "\n",
    "Related\n",
    "Başka hangi büyük ölçekli ön eğitimli dil modelleri var\n",
    "Bu modellerin özellikleri ve farkları nelerdir\n",
    "Hangi modeller daha hızlı ve verimli çalışıyor\n",
    "Yeni modellerin hangi uygulamalarda kullanılması önerilir\n",
    "Güncel gelişmeler ve yeni çıkan modeller hakkında bilgi var mı"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3114f87a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
