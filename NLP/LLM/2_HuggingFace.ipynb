{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fc26d72",
   "metadata": {},
   "source": [
    "### 2. HuggingFace Kütüphanesi\n",
    "\n",
    "### Neden HuggingFace?\n",
    "\n",
    "HuggingFace, doğal dil işleme (NLP) alanında devrim yaratmış bir açık kaynak platformudur. Özellikle `transformers` kütüphanesi sayesinde, BERT, GPT, T5 gibi karmaşık modelleri sadece birkaç satır kodla indirip kullanabiliyoruz.\n",
    "\n",
    "HuggingFace’in sunduğu temel bileşenler:\n",
    "\n",
    "- **`transformers`**: Önceden eğitilmiş modeller ve tokenizer’lar.\n",
    "- **`datasets`**: 30.000+ NLP veri kümesi.\n",
    "- **`tokenizers`**: Hızlı ve esnek tokenleştirme.\n",
    "- **`hub`**: Herkesin modellerini paylaştığı platform (model zoo).\n",
    "- **`accelerate`**: Dağıtık eğitim desteği.\n",
    "---\n",
    "### Ana Kavramlar\n",
    "\n",
    "- **Model**: Dil modeli (örneğin `bert-base-uncased`).\n",
    "- **Tokenizer**: Metni modele girdi olarak verilebilecek sayısal forma dönüştürür.\n",
    "- **Pipeline**: Ortak NLP görevleri (sınıflandırma, çeviri, soru-cevap) için hazır fonksiyonlar.\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "### Temel Özellikler:\n",
    "\n",
    "- 300.000+ önceden eğitilmiş model (BERT, RoBERTa, DeBERTa, etc.)\n",
    "- 2.000+ dil desteği (Türkçe de dahil)\n",
    "- Kolay fine-tuning ve dağıtım\n",
    "- Pipeline API: Satır kodla model kullan\n",
    "---\n",
    "\n",
    "\n",
    "### Kod Uygulaması: HuggingFace ile Metin Sınıflandırma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc22903",
   "metadata": {},
   "source": [
    "```\n",
    "%pip install transformers torch datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b34c7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "/Users/ibrahimediz/anaconda3/envs/nlp2/lib/python3.11/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hızlı kargo,kaliteli ürün => {'label': 'positive', 'score': 0.9931176900863647}\n",
      "Kalitesiz ürün iade ettim => {'label': 'negative', 'score': 0.9992490410804749}\n",
      "iyi ama fiyatına göre beklediğim gibi değil => {'label': 'negative', 'score': 0.9981688261032104}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\",model=\"savasy/bert-base-turkish-sentiment-cased\")\n",
    "\n",
    "yorumlar = [\n",
    "    \"Hızlı kargo,kaliteli ürün\",\n",
    "    \"Kalitesiz ürün iade ettim\",\n",
    "    \"iyi ama fiyatına göre beklediğim gibi değil\"\n",
    "]\n",
    "\n",
    "sonuclar = classifier(yorumlar)\n",
    "\n",
    "for yorum,sonuc in zip(yorumlar,sonuclar):\n",
    "    print(f\"{yorum} => {sonuc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1af7b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n) argument after ** must be a mapping, not method",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     10\u001b[39m inputs = tokenizer(\n\u001b[32m     11\u001b[39m     text,\n\u001b[32m     12\u001b[39m     return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m     max_length=\u001b[32m512\u001b[39m\n\u001b[32m     16\u001b[39m )\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     outputs = model(**\u001b[38;5;28minput\u001b[39m)\n\u001b[32m     21\u001b[39m probs = torch.softmax(outputs.logits,dim=-\u001b[32m1\u001b[39m)\n\u001b[32m     22\u001b[39m pred_label = torch.argmax(probs,dim=-\u001b[32m1\u001b[39m).item()\n",
      "\u001b[31mTypeError\u001b[39m: BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n) argument after ** must be a mapping, not method"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model_name = \"savasy/bert-base-turkish-sentiment-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "text = \"Çok iyi çok güzel çok ta iyi oldu tamam mı?\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "probs = torch.softmax(outputs.logits,dim=-1)\n",
    "pred_label = torch.argmax(probs,dim=-1).item()\n",
    "\n",
    "labels = [\"OLUMSUZ\",\"OLUMLU\"]\n",
    "print(\"Metin:\",text)\n",
    "print(f\"Tahmin:{labels[pred_label]},Güven:{probs[0][pred_label]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a39d697",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
