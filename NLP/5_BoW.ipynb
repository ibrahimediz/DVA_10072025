{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21d67161",
   "metadata": {},
   "source": [
    "\n",
    "## Bag of Words (BoW) Nedir?\n",
    "\n",
    "**Bag of Words**, bir metni (veya belgeyi) kelimelerinin sıklığına göre vektörleştiren en temel metin temsil yöntemidir.  \n",
    "Kelime sırası ve gramer dikkate alınmaz, sadece hangi kelimenin kaç kez geçtiği önemlidir.\n",
    "\n",
    "**Avantajları:**  \n",
    "- Basit ve hızlıdır  \n",
    "- Makine öğrenmesi algoritmalarıyla kolayca kullanılabilir  \n",
    "- Küçük veri setlerinde iyi çalışır\n",
    "\n",
    "**Dezavantajları:**  \n",
    "- Kelime sırası ve bağlamı kaybeder  \n",
    "- Çok büyük boyutlu vektörler oluşabilir  \n",
    "- Anlamsal ilişkileri yakalayamaz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c71a1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Bugün hava çok güzel ve güneşli.\",\n",
    "    \"Yarın hava yağmurlu olacak.\",\n",
    "    \"Bugün parka gittik ve çok eğlendik.\",\n",
    "    \"Yarın da parka gitmek istiyoruz.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcc664e",
   "metadata": {},
   "source": [
    "1. Python (Manuel BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06d6ad16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kelimeler (Vocab): ['bugün', 'da', 'eğlendik', 'gitmek', 'gittik', 'güneşli', 'güzel', 'hava', 'istiyoruz', 'olacak', 'parka', 've', 'yarın', 'yağmurlu', 'çok']\n",
      "BoW Vektörleri:\n",
      "Doc 1: [1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "Doc 2: [0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0]\n",
      "Doc 3: [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1]\n",
      "Doc 4: [0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def bow_python(docs):\n",
    "    # Tüm kelimeleri küçük harfe çevirip ayıkla\n",
    "    all_words = []\n",
    "    for doc in docs:\n",
    "        words = re.findall(r'\\w+', doc.lower(), re.UNICODE)\n",
    "        all_words.extend(words)\n",
    "    # Tüm kelimelerin kümesini al\n",
    "    vocab = sorted(set(all_words))\n",
    "    # Her doküman için kelime sıklıklarını hesapla\n",
    "    bow_vectors = []\n",
    "    for doc in docs:\n",
    "        words = re.findall(r'\\w+', doc.lower(), re.UNICODE)\n",
    "        counts = Counter(words)\n",
    "        vector = [counts.get(word, 0) for word in vocab]\n",
    "        bow_vectors.append(vector)\n",
    "    return vocab, bow_vectors\n",
    "\n",
    "vocab, bow_vectors = bow_python(documents)\n",
    "print(\"Kelimeler (Vocab):\", vocab)\n",
    "print(\"BoW Vektörleri:\")\n",
    "for i, vec in enumerate(bow_vectors):\n",
    "    print(f\"Doc {i+1}: {vec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e4fb7e",
   "metadata": {},
   "source": [
    "2. scikit-learn ile Bag of Words\n",
    "* Python’da en pratik ve yaygın yol, scikit-learn’ün CountVectorizer fonksiyonunu kullanmaktır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba681887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kelimeler (Vocab): ['bugün' 'da' 'eğlendik' 'gitmek' 'gittik' 'güneşli' 'güzel' 'hava'\n",
      " 'istiyoruz' 'olacak' 'parka' 've' 'yarın' 'yağmurlu' 'çok']\n",
      "BoW Vektörleri (sparse):\n",
      " [[1 0 0 0 0 1 1 1 0 0 0 1 0 0 1]\n",
      " [0 0 0 0 0 0 0 1 0 1 0 0 1 1 0]\n",
      " [1 0 1 0 1 0 0 0 0 0 1 1 0 0 1]\n",
      " [0 1 0 1 0 0 0 0 1 0 1 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase=True)\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "print(\"Kelimeler (Vocab):\", vectorizer.get_feature_names_out())\n",
    "print(\"BoW Vektörleri (sparse):\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714b653a",
   "metadata": {},
   "source": [
    "3. NLTK ile Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cc87201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kelimeler (Vocab): ['bugün', 'da', 'eğlendik', 'gitmek', 'gittik', 'güneşli', 'güzel', 'hava', 'istiyoruz', 'olacak', 'parka', 've', 'yarın', 'yağmurlu', 'çok']\n",
      "BoW Vektörleri:\n",
      "Doc 1: [1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "Doc 2: [0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0]\n",
      "Doc 3: [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1]\n",
      "Doc 4: [0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "# Her dokümanı tokenize et\n",
    "tokenized_docs = [nltk.word_tokenize(doc.lower()) for doc in documents]\n",
    "# Tüm kelimelerin kümesi\n",
    "vocab = sorted(set(word for doc in tokenized_docs for word in doc if word.isalpha()))\n",
    "# Her doküman için vektör\n",
    "bow_vectors = []\n",
    "for doc in tokenized_docs:\n",
    "    counts = Counter([word for word in doc if word.isalpha()])\n",
    "    vector = [counts.get(word, 0) for word in vocab]\n",
    "    bow_vectors.append(vector)\n",
    "\n",
    "print(\"Kelimeler (Vocab):\", vocab)\n",
    "print(\"BoW Vektörleri:\")\n",
    "for i, vec in enumerate(bow_vectors):\n",
    "    print(f\"Doc {i+1}: {vec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec90c2b",
   "metadata": {},
   "source": [
    "4. spaCy ile Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ec015d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kelimeler (Vocab): ['bugün', 'da', 'eğlendik', 'gitmek', 'gittik', 'güneşli', 'güzel', 'hava', 'istiyoruz', 'olacak', 'parka', 've', 'yarın', 'yağmurlu', 'çok']\n",
      "BoW Vektörleri:\n",
      "Doc 1: [1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "Doc 2: [0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0]\n",
      "Doc 3: [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1]\n",
      "Doc 4: [0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"tr\")  # Türkçe için boş model\n",
    "tokenized_docs = []\n",
    "for doc in documents:\n",
    "    tokens = [token.text.lower() for token in nlp(doc) if token.is_alpha]\n",
    "    tokenized_docs.append(tokens)\n",
    "\n",
    "vocab = sorted(set(word for doc in tokenized_docs for word in doc))\n",
    "bow_vectors = []\n",
    "for doc in tokenized_docs:\n",
    "    counts = Counter(doc)\n",
    "    vector = [counts.get(word, 0) for word in vocab]\n",
    "    bow_vectors.append(vector)\n",
    "\n",
    "print(\"Kelimeler (Vocab):\", vocab)\n",
    "print(\"BoW Vektörleri:\")\n",
    "for i, vec in enumerate(bow_vectors):\n",
    "    print(f\"Doc {i+1}: {vec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917f79eb",
   "metadata": {},
   "source": [
    "5. Zeyrek ve TRNLP ile BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75d0ba34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "APPENDING RESULT: <(bugün_Adv)(-)(bugün:advRoot_ST)>\n",
      "APPENDING RESULT: <(bugün_Noun_Time)(-)(bugün:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(hava_Adj)(-)(hava:adjectiveRoot_ST)>\n",
      "APPENDING RESULT: <(hav_Noun)(-)(hav:noun_S + a3sg_S + pnon_S + a:dat_ST)>\n",
      "APPENDING RESULT: <(hava_Noun)(-)(hava:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(çok_Det)(-)(çok:detRoot_ST)>\n",
      "APPENDING RESULT: <(çok_Postp_PCAbl)(-)(çok:postpRoot_ST)>\n",
      "APPENDING RESULT: <(çok_Adv)(-)(çok:advRoot_ST)>\n",
      "APPENDING RESULT: <(çok_Adj)(-)(çok:adjectiveRoot_ST)>\n",
      "APPENDING RESULT: <(güzel_Adj)(-)(güzel:adjectiveRoot_ST)>\n",
      "APPENDING RESULT: <(güzel_Adv)(-)(güzel:advRoot_ST)>\n",
      "APPENDING RESULT: <(güzel_Noun)(-)(güzel:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(ve_Conj)(-)(ve:conjRoot_ST)>\n",
      "APPENDING RESULT: <(Güneş_Noun_Prop)(-)(güneş:nounProper_S + a3sg_S + pnon_S + nom_ST + li:with_S + adjectiveRoot_ST)>\n",
      "APPENDING RESULT: <(güneş_Noun)(-)(güneş:noun_S + a3sg_S + pnon_S + nom_ST + li:with_S + adjectiveRoot_ST)>\n",
      "APPENDING RESULT: <(yarın_Adv)(-)(yarın:advRoot_ST)>\n",
      "APPENDING RESULT: <(yarmak_Verb)(-)(yar:verbRoot_S + vImp_S + ın:vA2pl_ST)>\n",
      "APPENDING RESULT: <(yar_Noun)(-)(yar:noun_S + a3sg_S + pnon_S + ın:gen_ST)>\n",
      "APPENDING RESULT: <(yar_Noun)(-)(yar:noun_S + a3sg_S + ın:p2sg_S + nom_ST)>\n",
      "APPENDING RESULT: <(yarı_Noun)(-)(yarı:noun_S + a3sg_S + n:p2sg_S + nom_ST)>\n",
      "APPENDING RESULT: <(yarın_Noun_Time)(-)(yarın:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(yarı_Adj)(-)(yarı:adjectiveRoot_ST + adjZeroDeriv_S + noun_S + a3sg_S + n:p2sg_S + nom_ST)>\n",
      "APPENDING RESULT: <(hava_Adj)(-)(hava:adjectiveRoot_ST)>\n",
      "APPENDING RESULT: <(hav_Noun)(-)(hav:noun_S + a3sg_S + pnon_S + a:dat_ST)>\n",
      "APPENDING RESULT: <(hava_Noun)(-)(hava:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(yağmur_Noun)(-)(yağmur:noun_S + a3sg_S + pnon_S + nom_ST + lu:with_S + adjectiveRoot_ST)>\n",
      "APPENDING RESULT: <(olmak_Verb)(-)(ol:verbRoot_S + acak:vFutPart_S + adjAfterVerb_S + aPnon_ST)>\n",
      "APPENDING RESULT: <(bugün_Adv)(-)(bugün:advRoot_ST)>\n",
      "APPENDING RESULT: <(bugün_Noun_Time)(-)(bugün:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(park_Noun)(-)(park:noun_S + a3sg_S + pnon_S + a:dat_ST)>\n",
      "APPENDING RESULT: <(Park_Noun_Prop)(-)(park:nounProper_S + a3sg_S + pnon_S + a:dat_ST)>\n",
      "APPENDING RESULT: <(parka_Noun)(-)(parka:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(gitmek_Verb)(-)(git:verbRoot_S + ti:vPast_S + k:vA1pl_ST)>\n",
      "APPENDING RESULT: <(gitmek_Verb)(-)(git:verbRoot_S + tik:vPastPart_S + adjAfterVerb_S + aPnon_ST)>\n",
      "APPENDING RESULT: <(gitmek_Verb)(-)(git:verbRoot_S + tik:vPastPart_S + noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(ve_Conj)(-)(ve:conjRoot_ST)>\n",
      "APPENDING RESULT: <(çok_Det)(-)(çok:detRoot_ST)>\n",
      "APPENDING RESULT: <(çok_Postp_PCAbl)(-)(çok:postpRoot_ST)>\n",
      "APPENDING RESULT: <(çok_Adv)(-)(çok:advRoot_ST)>\n",
      "APPENDING RESULT: <(çok_Adj)(-)(çok:adjectiveRoot_ST)>\n",
      "APPENDING RESULT: <(eğlenmek_Verb)(-)(eğlen:verbRoot_S + di:vPast_S + k:vA1pl_ST)>\n",
      "APPENDING RESULT: <(eğlenmek_Verb)(-)(eğlen:verbRoot_S + dik:vPastPart_S + adjAfterVerb_S + aPnon_ST)>\n",
      "APPENDING RESULT: <(eğlemek_Verb)(-)(eğle:verbRoot_S + n:vPass_S + verbRoot_S + di:vPast_S + k:vA1pl_ST)>\n",
      "APPENDING RESULT: <(eğlemek_Verb)(-)(eğle:verbRoot_S + n:vPass_S + verbRoot_S + dik:vPastPart_S + adjAfterVerb_S + aPnon_ST)>\n",
      "APPENDING RESULT: <(eğlenmek_Verb)(-)(eğlen:verbRoot_S + dik:vPastPart_S + noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(eğlemek_Verb)(-)(eğle:verbRoot_S + n:vPass_S + verbRoot_S + dik:vPastPart_S + noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(yarın_Adv)(-)(yarın:advRoot_ST)>\n",
      "APPENDING RESULT: <(yarmak_Verb)(-)(yar:verbRoot_S + vImp_S + ın:vA2pl_ST)>\n",
      "APPENDING RESULT: <(yar_Noun)(-)(yar:noun_S + a3sg_S + pnon_S + ın:gen_ST)>\n",
      "APPENDING RESULT: <(yar_Noun)(-)(yar:noun_S + a3sg_S + ın:p2sg_S + nom_ST)>\n",
      "APPENDING RESULT: <(yarı_Noun)(-)(yarı:noun_S + a3sg_S + n:p2sg_S + nom_ST)>\n",
      "APPENDING RESULT: <(yarın_Noun_Time)(-)(yarın:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(yarı_Adj)(-)(yarı:adjectiveRoot_ST + adjZeroDeriv_S + noun_S + a3sg_S + n:p2sg_S + nom_ST)>\n",
      "APPENDING RESULT: <(da_Conj)(-)(da:conjRoot_ST)>\n",
      "APPENDING RESULT: <(park_Noun)(-)(park:noun_S + a3sg_S + pnon_S + a:dat_ST)>\n",
      "APPENDING RESULT: <(Park_Noun_Prop)(-)(park:nounProper_S + a3sg_S + pnon_S + a:dat_ST)>\n",
      "APPENDING RESULT: <(parka_Noun)(-)(parka:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(gitmek_Verb)(-)(git:verbRoot_S + mek:vInf1_S + nounInf1Root_S + a3sgInf1_S + pnonInf1_S + nom_ST)>\n",
      "APPENDING RESULT: <(istemek_Verb)(-)(ist:verbRoot_VowelDrop_S + iyor:vProgYor_S + uz:vA1pl_ST)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeyrek ile lemmatize edilmiş kelimeler (her doküman):\n",
      "Doc 1: bugün hava çok güzel ve güneşli\n",
      "Doc 2: yarın hava yağmurlu olacak\n",
      "Doc 3: bugün parka gittik ve çok eğlendik\n",
      "Doc 4: yarın da parka gitmek istiyoruz\n",
      "BoW Vektörleri (Zeyrek lemmatize):\n",
      " [[1 0 0 0 0 1 1 1 0 0 0 1 0 0 1]\n",
      " [0 0 0 0 0 0 0 1 0 1 0 0 1 1 0]\n",
      " [1 0 1 0 1 0 0 0 0 0 1 1 0 0 1]\n",
      " [0 1 0 1 0 0 0 0 1 0 1 0 1 0 0]]\n",
      "Kelimeler (Vocab): ['bugün' 'da' 'eğlendik' 'gitmek' 'gittik' 'güneşli' 'güzel' 'hava'\n",
      " 'istiyoruz' 'olacak' 'parka' 've' 'yarın' 'yağmurlu' 'çok']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "try:\n",
    "    import zeyrek\n",
    "    analyzer = zeyrek.MorphAnalyzer()\n",
    "    zeyrek_available = True\n",
    "except ImportError:\n",
    "    print(\"Zeyrek kurulu değil.\")\n",
    "    zeyrek_available = False\n",
    "\n",
    "def lemmatize_with_zeyrek(text):\n",
    "    words = re.findall(r'\\w+', text.lower(), re.UNICODE)\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        analyses = analyzer.analyze(word)\n",
    "        if analyses and len(analyses) > 0 and len(analyses[0]) > 1 and analyses[0][1]:\n",
    "            lemma = analyses[0][1][0]\n",
    "        else:\n",
    "            lemma = word\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "if zeyrek_available:\n",
    "    lemmatized_docs = [' '.join(lemmatize_with_zeyrek(doc)) for doc in documents]\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(lemmatized_docs)\n",
    "    print(\"Zeyrek ile lemmatize edilmiş kelimeler (her doküman):\")\n",
    "    for i, doc in enumerate(lemmatized_docs):\n",
    "        print(f\"Doc {i+1}: {doc}\")\n",
    "    print(\"BoW Vektörleri (Zeyrek lemmatize):\\n\", X.toarray())\n",
    "    print(\"Kelimeler (Vocab):\", vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6128ef4",
   "metadata": {},
   "source": [
    "5. TRNLP ile Lemmatize Edilmiş Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c4ae79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRNLP ile lemmatize edilmiş kelimeler (her doküman):\n",
      "Doc 1: bugün hava çok güzel ve güneşli\n",
      "Doc 2: yarın hava yağmurlu olacak\n",
      "Doc 3: bugün parka gittik ve çok eğlendik\n",
      "Doc 4: yarın da parka gitmek istiyoruz\n",
      "BoW Vektörleri (TRNLP lemmatize):\n",
      " [[1 0 0 0 0 1 1 1 0 0 0 1 0 0 1]\n",
      " [0 0 0 0 0 0 0 1 0 1 0 0 1 1 0]\n",
      " [1 0 1 0 1 0 0 0 0 0 1 1 0 0 1]\n",
      " [0 1 0 1 0 0 0 0 1 0 1 0 1 0 0]]\n",
      "Kelimeler (Vocab): ['bugün' 'da' 'eğlendik' 'gitmek' 'gittik' 'güneşli' 'güzel' 'hava'\n",
      " 'istiyoruz' 'olacak' 'parka' 've' 'yarın' 'yağmurlu' 'çok']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "try:\n",
    "    from trnlp import TrnlpWord\n",
    "    trnlp_available = True\n",
    "except ImportError:\n",
    "    print(\"TRNLP kurulu değil.\")\n",
    "    trnlp_available = False\n",
    "\n",
    "def lemmatize_with_trnlp(text):\n",
    "    words = re.findall(r'\\w+', text.lower(), re.UNICODE)\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            tr_word = TrnlpWord()\n",
    "            tr_word.setword(word)\n",
    "            lemma = getattr(tr_word, \"lemma\", None) or getattr(tr_word, \"lemmas\", None)\n",
    "            if isinstance(lemma, list):\n",
    "                lemma = lemma[0] if lemma else word\n",
    "            elif not lemma:\n",
    "                lemma = word\n",
    "        except Exception:\n",
    "            lemma = word\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "if trnlp_available:\n",
    "    lemmatized_docs = [' '.join(lemmatize_with_trnlp(doc)) for doc in documents]\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(lemmatized_docs)\n",
    "    print(\"TRNLP ile lemmatize edilmiş kelimeler (her doküman):\")\n",
    "    for i, doc in enumerate(lemmatized_docs):\n",
    "        print(f\"Doc {i+1}: {doc}\")\n",
    "    print(\"BoW Vektörleri (TRNLP lemmatize):\\n\", X.toarray())\n",
    "    print(\"Kelimeler (Vocab):\", vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7992012",
   "metadata": {},
   "source": [
    "Açıklama:\n",
    "\n",
    "Her dokümandaki kelimeler önce lemmatize edilir.\n",
    "Sonra, lemmatize edilmiş metinler ile klasik BoW vektörü oluşturulur.\n",
    "Bu yöntem, Türkçe’de kök/ek farklılıklarından kaynaklanan BoW şişmesini azaltır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e75758f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
